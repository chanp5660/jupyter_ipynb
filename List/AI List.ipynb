{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516682c0-b7f9-44fd-ae48-55c3011434f9",
   "metadata": {},
   "source": [
    "---  \n",
    "layout: post  \n",
    "current: post  \n",
    "cover:  assets/built/images/AI_List.png  \n",
    "navigation: True  \n",
    "title: AI List   \n",
    "date: 2023-02-13 00:00:00 +0900  \n",
    "tags: [ai]  \n",
    "class: post-template  \n",
    "subclass: 'post tag-python'  \n",
    "author: chanp5660  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce222ba-6625-42c2-a0ab-6785c71cd62a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AI 목차\n",
    "\n",
    "- [지도 학습 (Supervised Learning)](https://chanp5660.github.io/지도학습)\n",
    "    - 회귀 문제\n",
    "        - 선형 회귀 (Linear Regression)\n",
    "        - [의사결정트리 (Decision Tree)](https://chanp5660.github.io/의사결정트리)\n",
    "        - 랜덤 포레스트 (Random Forest)\n",
    "        - 서포트 벡터 머신 (Support Vector Machine, SVM)\n",
    "        - 신경망 (Neural Network)\n",
    "    - 분류 문제\n",
    "        - 로지스틱 회귀(Logistic Regression)\n",
    "        - 의사결정 나무(Decision Tree)  \n",
    "        - k-최근접 이웃(K-Nearest Neighbor)\n",
    "        - 나이브 베이즈(Naive Bayes)\n",
    "        - 서포트 벡터 머신(Support Vector Machine)\n",
    "        - 인공신경망(Artificial Neural Network)\n",
    "- [비지도 학습 (Unsupervised Learning)](https://chanp5660.github.io/비지도학습)\n",
    "- 강화 학습 (Reinforcement Learning)\n",
    "- 준지도 학습 (Semi-supervised Learning)\n",
    "- 전이 학습 (Transfer Learning)\n",
    "- 생성 모델 (Generative Models)\n",
    "- 딥 러닝 (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cdbaa-d000-49db-a59b-8a700f7f33da",
   "metadata": {},
   "source": [
    "--- \n",
    "• Gradient Descent란?  \n",
    "• Loss Surface란?  \n",
    "• Attention이란?  \n",
    "• Transformer란?  \n",
    "• Collaborative filtering이란?  \n",
    "• Few-Shot Learning이란?  \n",
    "• Federated Learning이란?  \n",
    "• SVD란?  \n",
    "• 중심극한정리란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc6b43-dd78-4fc5-b4d8-60396d31f58f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------------------\n",
    "\n",
    "\n",
    "## 포스팅 됨 \n",
    "\n",
    "- [서포트벡터머신(SVM)](https://chanp5660.github.io/서포트벡터머신(SVM))\n",
    "- [로지스틱 회귀분석](https://chanp5660.github.io/로지스틱회귀)\n",
    "- [결정경계](https://chanp5660.github.io/결정경계)\n",
    "- [손실함수](https://chanp5660.github.io/손실함수)\n",
    "- [선형분류](https://chanp5660.github.io/선형분류)\n",
    "\n",
    "## 포스팀 안됨\n",
    "\n",
    "> 이동하면서 볼 수 있도록 공부할만한 사이트 링크 걸어둔다.\n",
    "\n",
    "- [GridSearch](https://velog.io/@hyunicecream/GridSearchCV란-어떻게-사용할까)\n",
    "- [이진분류](https://076923.github.io/posts/Python-pytorch-12/)\n",
    "- [다중분류](https://076923.github.io/posts/Python-pytorch-13/)\n",
    "- [소프트맥스 함수](https://syj9700.tistory.com/38)\n",
    "- [확률적경사하강법](https://mangkyu.tistory.com/62)\n",
    "- [ML-데이터수집]()\n",
    "- [Log Loss](https://seoyoungh.github.io/machine-learning/ml-logloss/)\n",
    "- [의사결정트리](https://wooono.tistory.com/104)\n",
    "    - [동영상1](https://www.youtube.com/watch?v=xki7zQDf74I&list=PLpIPLT0Pf7IoTxTCi2MEQ94MZnHaxrP0j&index=24)\n",
    "    - [동영상2](https://www.youtube.com/watch?v=2Rd4AqmLjfU&list=PLpIPLT0Pf7IoTxTCi2MEQ94MZnHaxrP0j&index=23)\n",
    "- [엔트로피]()\n",
    "- [gini index]()\n",
    "- [bias and variance trade-off]()\n",
    "- [bagging;bootstrap aggregating](https://m.blog.naver.com/muzzincys/220201299384)\n",
    "- [boosting]\n",
    "- [out-of-bag]\n",
    "- [앙상블;ensemble]\n",
    "    - 부스팅\n",
    "        - [adaboosting]\n",
    "        - [Gradient Boosting]\n",
    "- [ROC AUC]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac7263-aae2-4625-8edc-b6ba7e29c623",
   "metadata": {},
   "source": [
    "# 자주 쓰이는 용어 적어봄\n",
    "\n",
    "> 우선 적어놓고 겹치는 것을 제거해 나가고 추가적인 포스팅이 필요하면 위로 올린다.\n",
    "\n",
    "- 데이터셋 (Dataset): 머신러닝 모델의 학습을 위해 사용되는 데이터의 집합입니다.\n",
    "- 특성 (Feature): 머신러닝 모델의 입력값으로 사용되는 데이터의 특징을 나타내는 변수입니다.\n",
    "- 레이블 (Label): 지도학습에서 모델이 예측해야 하는 정답값입니다.\n",
    "- 모델 (Model): 머신러닝 알고리즘이 학습하여 만들어지는 예측 모형입니다.\n",
    "- 학습 (Training): 머신러닝 모델이 데이터를 이용하여 예측을 하기 위해 데이터에 적합한 가중치와 파라미터를 조정하는 과정입니다.\n",
    "- 평가 (Evaluation): 모델이 학습된 후에 모델의 예측 능력을 평가하기 위한 과정입니다.\n",
    "- 예측 (Prediction): 모델이 학습한 내용을 바탕으로 새로운 입력값에 대해 출력값을 예측하는 과정입니다.\n",
    "- 분류 (Classification): 지도학습에서 레이블이 범주형인 경우, 입력값을 범주형으로 분류하는 문제입니다.\n",
    "- 회귀 (Regression): 지도학습에서 레이블이 수치형인 경우, 입력값과 수치형 출력값 사이의 관계를 예측하는 문제입니다.\n",
    "- 군집화 (Clustering): 비지도학습에서 데이터를 유사한 그룹으로 나누는 문제입니다.\n",
    "- 차원 축소 (Dimensionality Reduction): 데이터의 특성을 유지하면서 데이터의 차원을 줄이는 방법입니다.\n",
    "- 과적합 (Overfitting): 학습 데이터에 과도하게 적합하게 모델이 학습되어 새로운 데이터에 대한 예측 능력이 떨어지는 상황입니다.\n",
    "- 일반화 (Generalization): 학습 데이터를 바탕으로 일반적인 규칙을 학습하여, 새로운 데이터에 대해서도 정확한 예측을 할 수 있는 능력입니다.\n",
    "- 하이퍼파라미터 (Hyperparameter): 모델의 학습과정을 제어하는 변수로, 학습 전에 설정되며, 학습과정에서는 수정되지 않는 파라미터입니다. 예를 들면 학습률, 배치 크기, 에포크 등이 있습니다.\n",
    "- 최적화 (Optimization): 모델의 성능을 향상시키기 위해 가중치나 하이퍼파라미터를 조정하는 과정\n",
    "- Loss function: 모델의 예측값과 실제 값의 차이를 나타내는 함수로, 모델의 성능을 측정하는 데 사용됩니다. 모델을 훈련시키는 데 사용됩니다.\n",
    "- Regularization: 모델이 과적합(overfitting)되지 않도록 제한하는 방법으로, L1 정규화(L1 regularization)와 L2 정규화(L2 regularization)가 있습니다.\n",
    "- Gradient descent: Loss function을 최소화하기 위한 최적화 알고리즘으로, 기울기를 계산하여 경사를 따라 최적값을 찾아가는 방법입니다.\n",
    "- Hyperparameters: 모델을 구성하는 파라미터가 아니라 모델의 학습에 사용되는 파라미터로, 예를 들어 학습률(learning rate)과 같은 하이퍼파라미터가 있습니다.\n",
    "- Ensemble learning: 여러 개의 모델을 조합하여 더 강력한 모델을 만드는 방법으로, 예를 들어 랜덤 포레스트(Random Forest)와 부스팅(Boosting)이 있습니다.\n",
    "- Transfer learning: 미리 학습된 모델을 가져와 새로운 문제에 대해 재사용하는 방법입니다.\n",
    "- Overfitting: 모델이 훈련 데이터에 너무 맞추어져서 새로운 데이터에 대해 일반화하기 어려워지는 현상입니다.\n",
    "- Underfitting: 모델이 너무 간단해서 데이터의 복잡성을 충분히 반영하지 못하는 현상입니다.\n",
    "- Validation set: 모델의 학습에 사용되는 데이터를 훈련 데이터와 검증 데이터로 나누는데, 검증 데이터를 validation set이라고 부릅니다. 모델의 학습에 사용되지 않고 검증에만 사용됩니다.\n",
    "- Cross-validation: 데이터를 여러 개의 폴드로 나누어 각 폴드를 검증 데이터로 사용하여 모델의 일반화 성능을 검증하는 방법입니다.\n",
    "- Early stopping: 모델이 과적합되는 것을 방지하기 위해 일정 epoch 이상 성능이 개선되지 않으면 학습을 중단하는 방법입니다.\n",
    "- One-hot encoding: 범주형 데이터를 수치형 데이터로 변환하는 방법 중 하나로, 각 범주를 나타내는 하나의 열(column)을 추가하여 0 또는 1의 값을 가지도록 합니다.\n",
    "- Bagging (Bootstrap Aggregating): 부트스트랩 샘플링을 통해 여러 개의 모델을 만들고, 이들의 예측값을 평균 또는 다수결 투표를 통해 결합하는 앙상블 방법입니다. 랜덤 포레스트(Random Forest)가 대표적인 예입니다.\n",
    "- Boosting: 약한 학습기를 여러 개 결합하여 강한 학습기를 만드는 앙상블 방법입니다. 이전 학습기가 잘못 예측한 샘플에 가중치를 높여 새로운 학습기에 보내는 방식입니다. 대표적인 예는 그래디언트 부스팅(Gradient Boosting)과 XGBoost(Extreme Gradient Boosting)가 있습니다.\n",
    "- Regularization: 과적합(Overfitting)을 방지하기 위한 기법입니다. L1, L2 규제와 같은 방법으로 모델의 가중치를 조정하거나, 드롭아웃(Dropout)처럼 무작위로 일부 노드를 제거하여 학습하는 방법 등이 있습니다.\n",
    "- Hyperparameter: 모델을 학습할 때 사전에 정해진 값으로써, 최적의 값이 모델의 성능에 큰 영향을 미칩니다. 예를 들어 의사결정트리에서는 최대 깊이(max depth)와 최소 분리 노드 수(min_samples_split) 등이 하이퍼파라미터입니다.\n",
    "- Cross-validation: 데이터를 학습용(train)과 검증용(validation)으로 나누어 학습을 진행하는 방식입니다. K-fold cross-validation은 데이터를 K개로 나누어 각각을 검증 데이터로 사용하며, 각 경우마다 모델을 다르게 학습시켜 평균값을 구하는 방식입니다.\n",
    "\n",
    "\n",
    "- 정보이득(IG)\n",
    "- 엔트로피"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
