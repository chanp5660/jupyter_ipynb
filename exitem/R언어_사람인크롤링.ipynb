{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc673c0-3a19-4b6b-847a-4eac47d79761",
   "metadata": {},
   "source": [
    "---  \n",
    "layout: post  \n",
    "current: post  \n",
    "cover:  assets/built/images/R-logo.png  \n",
    "navigation: True  \n",
    "title: R언어를 이용한 사람인 크롤링   \n",
    "date: 2019-05-19 00:19:00 +0900  \n",
    "tags: [exitem]  \n",
    "class: post-template  \n",
    "subclass: 'post tag-python'  \n",
    "author: chanp5660  \n",
    "---  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275654ee-ac3a-4459-aaaf-112e97ee1822",
   "metadata": {},
   "source": [
    "# R언어를 이용한 사람인 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae36c7-1b1c-40f8-b8e8-4a6592057110",
   "metadata": {},
   "source": [
    "## 어떤 데이터인가?\n",
    "\n",
    "국내 채용공고 사이트의 **공고(구인)** \n",
    "\n",
    "> 회사명, 공고제목, 즐겨찾기 수, 조회수, 경력, 학력, 나이/성별, 급여, 회사명, 공고제목, 즐겨찾기 수, 조회수, 경력, 학력, 나이/성별, 급여, 근무형태, 직급/직책, 근무일시, 근무지역, 시작일, 마감일, 우대사항, 지원금/보험, 급여제도, 선물, 교육/생활, 근무 환경, 출퇴근, 리프레시, 조직문화, 홈페이지접속, 필수사항, 자사양식다운수, 근무형태, 직급/직책, 근무일시, 근무지역, 시작일, 마감일, 우대사항, 지원금/보험, 급여제도, 선물, 교육/생활, 근무 환경, 출퇴근, 리프레시, 조직문화, 홈페이지접속, 필수사항, 자사양식다운수 , 그 외의 **기업정보** 도 있다.\n",
    "\n",
    "## 이 데이터가 왜 유용한가?\n",
    "\n",
    "요즘에는 구인을 할때에는 사람들은 인터넷, 앱 사용 등 구인구직 사이트를 이용하여 기업의 정보, 채용공고의 정보를 얻습니다.\n",
    "현재 나와있는 사이트나 앱 등도 잘 사용되고 있지만 데이터화하여 추천 정보나 공고의 정보를 다른 방향으로 잘 표현 할 수 있다.\n",
    "\n",
    "* * *\n",
    "\n",
    "### 실제 크롤링한 데이터베이스로 활용한 사례\n",
    "\n",
    "실제로 '사람인'이 처음에 채용공고를 얻는 방법은 크롤링이었습니다. '잡코리아'의 사이트를 크롤링하여 추가적으로 방법을 바꾸었더니 아래와 같은 결과를 얻었습니다. 2016년 사람인 광고 수상작에서 발췌했습니다.\n",
    "\n",
    "![사람인1](https://user-images.githubusercontent.com/46266247/56227940-919d6500-60b1-11e9-926e-f805367a0027.JPG)\n",
    "\n",
    "![사람인](https://user-images.githubusercontent.com/46266247/56227943-93672880-60b1-11e9-870f-7518bd6bf021.JPG)\n",
    "\n",
    "\n",
    "사진에서 보듯이 크롤링을 통해 채용공고나 기업의 정보를 데이터화하여 기능의 추가만으로 원래 있던 프로그램들을 향상 시킬 수 있습니다. 그 후 '잡코리아'도 시스템을 변화하여 '사람인'과 이용하는 정도가 비슷해졌습니다.\n",
    "\n",
    "사례에서 볼 수 있듯이 **채용공고를 데이터화 하여 가지고 있다면 다른 방향으로 생각해 볼 수 있는 유용한 데이터라고 생각합니다.**\n",
    "\n",
    "### 크롤링 할 때 주의해야 할 실제 사건\n",
    "\n",
    "하지만 **법적문제**도 생각해야한다는 것을 조사하면서 알았습니다.\n",
    "\n",
    "![사람인_잡코리아](https://user-images.githubusercontent.com/46266247/56228216-3ae45b00-60b2-11e9-9772-2b656c9d0ce5.JPG)\n",
    "\n",
    "'사람인'에서 크롤링 할 때 '잡코리아'에서 IP차단을 했지만 프로그램을 이용해 **IP를 바꿔가면서** 크롤링 했고 그 데이터를 통해 '잡코리아'를 뛰어넘어 **막대한 경제적 피해** 를 줬기 때문에 '잡코리아'가 승소하게 되었다.\n",
    "\n",
    "주의해야 하는 것 : 해당 사이트 하단에 표시가 되어 있거나 서비스 공약에 거부 의사가 있다면 주의, IP차단 의사가 있다면 주의.\n",
    "무조건 불법이 아닌 막대한 피해를 줬을 때 소송하여 불법으로 인정됩니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## 실제 구현 \n",
    "R언어를 이용해서 직접 구현한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f698fa-417e-4133-94a7-dedb3b76e8f8",
   "metadata": {},
   "source": [
    "## 채용공고 정보 수집(구인구직 사이트에서 크롤링)\n",
    "\n",
    "### 이용 사이트 : [사람인](http://www.saramin.co.kr/)<br>추가할 사이트 : [워크넷](https://www.work.go.kr/seekWantedMain.do) <br>관련있지만 크롤링하는데에 문제가 있어 이용하지 못하는 사이트 : [잡투게더](http://www.jobtogether.net/), [잡코리아](http://www.jobkorea.co.kr/)<br>정보가 적고 사용자가 적은 사이트 : [인쿠르트](http://www.incruit.com/), [커리어](http://www.career.co.kr/)\n",
    "\n",
    "* * *\n",
    "\n",
    "### 사람인\n",
    "요약 \n",
    "- 자바스크립트로 이루어져있어 [phantomjs](http://phantomjs.org/download.html) 프로그램을 이용하여 html 파일로 저장한다.\n",
    "\n",
    "- html을 **rvest** 패키지를 이용해서 크롤링\n",
    "\n",
    "  - 총 페이지의 수를 구함\n",
    "  - 각 페이지의 공고의 url을 구함\n",
    "  - 각 url의 정보를 크롤링\n",
    "#### 데이터를 가져와야하는 사이트 사진1[이동](https://bit.ly/2wEYoFe)\n",
    "\n",
    "![공채의 명가](https://user-images.githubusercontent.com/46266247/56227133-b98bc900-60af-11e9-9eb6-ccc56489f59c.JPG)\n",
    "\n",
    "#### 데이터를 가져와야하는 사이트 사진2[이동](https://bit.ly/2Gns9fs)\n",
    "\n",
    "![공고1](https://user-images.githubusercontent.com/46266247/56227194-db854b80-60af-11e9-9ed3-b2854bfe92b8.JPG)\n",
    "\n",
    "* * *\n",
    "\n",
    "![공고2](https://user-images.githubusercontent.com/46266247/56227197-dd4f0f00-60af-11e9-8c58-1e4cebc10496.JPG)\n",
    "\n",
    "\n",
    "#### 과정\n",
    "\n",
    "- 'PantomJs' 프로그램을 사용해서 html 파일로 저장.\n",
    "![JShtml](https://user-images.githubusercontent.com/46266247/56230134-9d3f5a80-60b6-11e9-9398-fa49e08fa2f1.png)\n",
    "\n",
    "  - 총 개수, 총 페이지수를 구해서 모든 공고의 상세보기 URL 번호 저장. [num_total.txt파일](https://github.com/chanp5660/R_chanp5660/blob/master/Project/Crawling/Saramin/result/num_total.txt)\n",
    "  \n",
    "    - ```http://www.saramin.co.kr/zf_user/jobs/relay/view?isMypage=no&rec_idx=```**36015093**```&recommend_ids=eJxVzrsNgFAMQ9Fp6F%2FifGsGYf8tgM4uj65iBXUyzP0p86tvfAQmmFstjGRmghlrTNdqWq1l6vuE6LspjGHOSG3IbZ39%2BQKwQS%2BW&view_type=public-recruit&gz=1&t_ref=public-recruit#seq=0```\n",
    "\n",
    "  - 해당 URL을 이용해서 각 html 파일로 저장.[Html파일](https://github.com/chanp5660/R_chanp5660/tree/master/Project/Crawling/Saramin/result/Html_files)\n",
    "\n",
    "- Html 파일을 'rvest' 패키지를 사용해서 읽는다.\n",
    "  - 필요한 정보들을 rvest와 gsub, grep, gregexpr, strsplit등 텍스트를 다루는 함수, match, Sys.sleep, write.csv등 그 외의 필요한 함수들을 사용하여 분류하여 저장.\n",
    "\n",
    "- 데이터를 500개씩 나누어 저장.[csv파일](https://github.com/chanp5660/R_chanp5660/tree/master/Project/Crawling/Saramin/result/Data_csv)\n",
    "\n",
    "#### 자동화\n",
    "\n",
    "위의 정보 중 URL 번호를 저장했는데 앞으로 받는 정보 중 겹치는 정보는 여기서 판별한다. 매일 반복해서 정보를 얻어 추가 시킬 수 있다.\n",
    "\n",
    "### 이번 느낀점\n",
    "1. 데이터를 읽어오는 것.\n",
    "2. 데이터를 구별하고 문자열 정리.\n",
    "3. 분류의 기준을 정하는 것.\n",
    "\n",
    "시간이 오래걸렸지만 이번 경험으로 앞으로는 데이터를 얻는데 큰 도움이 된 것 같습니다.\n",
    "\n",
    "- 크롤링이 크게 어렵다고 생각하지 않았는데 실제 기업들을 상대로 크롤링을 하려고 하니 평소에 해오던 크롤링과는 달랐습니다. 생각보다 시간도 많이 들고 방법도 공부가 많이 필요했습니다. \n",
    "- 그동안 저는 만들어진 데이터를 사용하기만 했습니다. 이번에는 직접 데이터를 만들어 보았습니다. 데이터를 분석하는 것도 중요한데 데이터가 없으면 아무것도 할 수가 없다는 것을 깨닫고 데이터의 소중함을 알 수 있는 경험이 되었습니다.\n",
    "\n",
    "### 다음 계획\n",
    "\n",
    "1. '사람인' 사이트에서 전에 했던 데이터까지 얻는 곳을 알아 두어서 추가로 정보를 저장.\n",
    "2. '워크넷(고용노동부)' 사이트까지 자동화 시키는 것.(한개의 사이트로는 부족하다고 생각.)\n",
    "3. 각 사이트마다 형태가 다를 텐데 그 분류를 비슷하게 하여 가능한 색인이 잘 되도록 수정.\n",
    "\n",
    "\n",
    "## R코드\n",
    "\n",
    "{% gist https://gist.github.com/chanp5660/89c987660cc2eb18641f8506d489a9a9.js %}\n",
    "\n",
    "\n",
    "## 결과 자료  \n",
    "\n",
    "[result.zip](https://github.com/chanp5660/chanp5660.github.io/files/9128002/result.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f7d18-78a4-43d5-a945-f0e64a4145f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
