{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f448b5-f099-404a-82ab-db75e88401c4",
   "metadata": {},
   "source": [
    "---  \n",
    "layout: post  \n",
    "current: post  \n",
    "cover:  assets/built/images/Attention.png  \n",
    "navigation: True  \n",
    "title: TIL-Attention   \n",
    "date: 2023-03-11 00:00:00 +0900  \n",
    "tags: [exitem,TIL]  \n",
    "class: post-template  \n",
    "subclass: 'post tag-python'  \n",
    "author: chanp5660  \n",
    "---  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335bb579-da42-4cf9-a3a4-ece5488bd4a8",
   "metadata": {},
   "source": [
    "# Attention이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bc713-9ce1-4feb-8a90-3e051df03ca4",
   "metadata": {},
   "source": [
    "- 머신러닝에서 자연어 처리나 컴퓨터 비전 분야에서 주로 사용되는 기술로, 입력 시퀀스의 중요한 부분에 더 많은 가중치를 부여하여 출력을 생성하는 방식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c61c57-2a91-463b-8161-6ee38ab1c106",
   "metadata": {},
   "source": [
    "## 사용 계기\n",
    "- 일반적으로, 시퀀스 데이터는 고정된 길이를 가지는 벡터로 변환되어 모델에 입려됩니다.\n",
    "    - 그러나 이러한 고정 길이 벡터는 데이터의 모든 정보를 포착하지 못할 수 있습니다.\n",
    "- 이러한 문제를 해결하기 위해 입력 시퀀스의 모든 위치에 대한 가중치를 계산합니다.\n",
    "    - 이를 기반으로 가중합을 계산하여 입력 시퀀스의 특정 위치에 더 많은 주의를 기울입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fcf0f5-6a88-461e-8f3f-59ff6784ac92",
   "metadata": {},
   "source": [
    "## 사용 \n",
    "- 자연어 처리 분야에서 Attention은 번역, 요약, 질문 답변, 감정 분석 등 다양한 테스트에서 사용됩니다.\n",
    "- 예를 들어 번역에서, 입력 문장의 각 단어에 대한 가중치를 계산합니다.\n",
    "    - 이를 기반으로 출력 문장의 각 단어를 생성합니다.\n",
    "    - 이를 통해 모델은 번역에 필요한 정보를 보다 효과적으로 추출할 수 있습니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569aae1e-722b-420f-80c0-07bd71ae2bb1",
   "metadata": {},
   "source": [
    "## 구현\n",
    "- 일반적으로 소프트맥스 함수와 내적 연산을 이용하여 가중치를 계산합니다.\n",
    "- 가중치를 계산하는 방법은 다양한 종류가 있습니다.\n",
    "    - Scaled Dot-Product Attention : 입력된 Query, Key, Value에 대해 내적(dot product)을 계산하고, 이를 스케일링하여 softmax 함수를 적용한 가중치를 Value에 곱해서 출력하는 방식입니다.\n",
    "    - Multi-Head Attention : 입력된 Query, Key, Value를 여러 개의 서로 다른 헤드(Head)로 분리하여 각각의 헤드에서 Scaled Dot-Product Attention을 계산하고, 이를 다시 합쳐서 출력하는 방식입니다.\n",
    "    - 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538f40d-1ebd-4f4c-be97-14889f1b1cdc",
   "metadata": {},
   "source": [
    "## 장점\n",
    "- 이전의 RNN, LSTM 등의 모델보다 더울 정확하고 효율적인 결과를 도출할 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78287280-6b78-4a54-9044-6e62b569e69f",
   "metadata": {},
   "source": [
    "## 과정\n",
    "1. Query, Key, Value 생성 : 입력 시퀀스를 표현하는 벡터를 Query, Key, Value로 분리합니다.\n",
    "    - Query는 출력에 대한 정보를 제공하는 벡터입니다.\n",
    "    - Key 는 입력 시퀀스 내의 각 원소에 대한 정보를 제공하는 벡터입니다.\n",
    "    - Value 는 입력 시퀀스 내의 원소를 표현하는 벡터입니다.\n",
    "2. Attention 가중치 계산 : Query와 Key 사이의 내적을 계산하여 유사도를 측정합니다.\n",
    "    - 이를 소프트맥스 함수를 이용하여 가중치로 변환합니다.\n",
    "    - 가중치는 입력 시퀀스 내의 각 원소에 대한 중요도를 나타냅니다.\n",
    "3. 가중합 계산 : 입력 시퀀스 내의 Value 벡터와 가중치를 곱하여 가중합을 계산합니다.\n",
    "    - 이를 통해 입력 시퀀스 내에서 주목할 부분이 강조됩니다.\n",
    "4. 출력 생성 : 가중합을 기반으로 출력 벡터를 생성합니다.\n",
    "    - 이때, 가중합의 결과를 어떻게 활용할지는 태스크에 따라 다릅니다.\n",
    "    - 예를 들어, 번역에서는 출력 문장의 각 단어를 생성하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50aa925-a551-424d-adb2-fe608fbeeff1",
   "metadata": {},
   "source": [
    "## 다른 기술과 조합하여 사용\n",
    "- Transformer 모델은 Attention 기반의 Encoder와 Decoder를 조합하여 사용하는 모델입니다.\n",
    "    - 이를 통해 자연어 처리 분야에서 최고 수준의 성능을 보여주고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1f7fb-bada-4396-9dc6-1a47b242290e",
   "metadata": {},
   "source": [
    "## 컴퓨터 비전 분야에서도 활발하게 사용\n",
    "- 예를 들어, 이미지 캡셔닝에서 이미지의 각 부분에 대한 가중치를 계산하여 캡션을 생성하는 데 사용됩니다.\n",
    "    - 이를 통해 이미지와 텍스트 정보를 결합하여 보다 정확한 결과를 도출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0edd3f-02f0-4222-9400-80ceb3e6a7c5",
   "metadata": {},
   "source": [
    "## 학습된 가중치를 시각화\n",
    "- 장점 : 학습된 가중치를 시각화하여 모델이 어떤 부분에 주목하고 있는지 확인할 수 있습니다.\n",
    "    - 이는 모델의 해석 가능성을 높여주며, 모델의 개선에도 도움이 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
