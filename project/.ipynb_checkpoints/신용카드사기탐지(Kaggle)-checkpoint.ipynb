{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post  \n",
    "current: post  \n",
    "cover:  assets/built/images/신용카드사기탐지.png  \n",
    "navigation: True  \n",
    "title: 신용카드 사기 탐지  \n",
    "date: 2023-04-02 00:00:00 +0900  \n",
    "tags: [project]  \n",
    "class: post-template  \n",
    "subclass: 'post tag-python'  \n",
    "author: chanp5660  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 신용카드 사기 탐지 (Credit Card Fraud Detection)\n",
    "\n",
    "[kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) 에서 기존의 분석 경험 쌓기 + 프로그래밍 스킬 + 분석할 때 사용되는 영어 공부를 같이 진행합니다. 확실하게 진행되는 것이 우선순위인 속도로 진행됩니다.\n",
    "- [janiobachmann](https://www.kaggle.com/code/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets) 님이 작성해주신 CODE를 필사합니다. 감사합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3689760c-41f8-4a33-9c96-3fd17803950e",
    "_uuid": "3e0ad409d438c7c68ea6a76700a1e964a357453f"
   },
   "source": [
    "<h1 align=\"center\"> Credit Fraud Detector </h1>  \n",
    "\n",
    "<h1 align=\"center\"> 신용카드 사기 탐지 </h1>\n",
    "\n",
    "**Note:** There are still aspects of this kernel that will be subjected to changes. I've noticed a recent increase of interest towards this kernel so I will focus more on the steps I took and why I took them to make it clear why I took those steps.\n",
    "\n",
    "**메모:** 이 커널에는 추가로 변경될 수 있는 측면이 여전히 있습니다. 최근 이 커널에 대한 관심이 증가하고 있다는 것을 알게 되었기 때문에 취한 조치와 그 이유를 명확히 하기 위해 취한 조치에 더 집중하겠습니다.\n",
    "\n",
    "<h2>Before we Begin:  </h2>\n",
    "If you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards this subject and will look for more efficient ways so that our models are able to detect more accurately both fraud and non-fraud transactions.\n",
    "\n",
    "## 시작하기 전에\n",
    "만약 이 작업이 좋다면, 이 커널을 추천해주세요. 이 커널은 제가 이 주제에 대해 더 심층적인 연구를 수행하도록 동기를 부여하고 모델이 사기 및 비사기 거래를 더 정확하게 탐지할 수 있도록 더 효율적인 방법을 찾을 것입니다.\n",
    "\n",
    "\n",
    "<h2> Introduction </h2>\n",
    "In this kernel we will use various predictive models to see how accurate they  are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset, the features are scaled and the names of the features are not shown due to privacy reasons. Nevertheless, we can still analyze some important aspects of the dataset. Let's start!\n",
    "\n",
    "## 소개\n",
    "이 커널은 다양한 예측 모델을 사용하여 **거래가 정상적인 지불인지 사기인지 탐지하는데 얼마나 정확한지 확인**할 것입니다. 데이터 세트에 설명된 대로 요인의 크기가 조정되고 개인 정보 보호 문제로 인해 요인의 이름이 표시되지 않습니다. 그럼에도 불구하고, 우리는 여전히 데이터 세트의 몇 가지 **중요한 측면**을 분석할 수 있습니다. 시작해봅시다.\n",
    "\n",
    "\n",
    "<h2> Our Goals: </h2>\n",
    "<ul>\n",
    "<li> Understand the little distribution of the \"little\" data that was provided to us. </li>\n",
    "<li> Create a 50/50 sub-dataframe ratio of \"Fraud\" and \"Non-Fraud\" transactions. (NearMiss Algorithm) </li>\n",
    "<li> Determine the Classifiers we are going to use and decide which one has a higher accuracy. </li>\n",
    "<li>Create a Neural Network and compare the accuracy to our best classifier. </li>\n",
    "<li>Understand common mistaked made with imbalanced datasets. </li>\n",
    "</ul>\n",
    "\n",
    "## 목표\n",
    "- 우리에게 제공된 작은 데이터의 작은 분포를 이해합니다.\n",
    "- \"Fraud\" 와 \"Non-Fraud\" 거래의 50/50 하위 데이터 프레임 비율을 만듭니다. (NearMiss Algorithm)\n",
    "- 사용할 분류자를 결정하고 정확도가 높은 분류자를 결정합니다.\n",
    "- 신경망을 만들고 우리의 최고 분류기와 정확도를 비교합니다.\n",
    "- 불균형 데이터셋으로 인한 일반적인 실수를 이해합니다.\n",
    "\n",
    "\n",
    "\n",
    "<h2> Outline: </h2>\n",
    "I. <b>Understanding our data</b><br>\n",
    "a) [Gather Sense of our data](#gather)<br><br>\n",
    "\n",
    "II. <b>Preprocessing</b><br>\n",
    "a) [Scaling and Distributing](#distributing)<br>\n",
    "b) [Splitting the Data](#splitting)<br><br>\n",
    "\n",
    "III. <b>Random UnderSampling and Oversampling</b><br>\n",
    "a) [Distributing and Correlating](#correlating)<br>\n",
    "b) [Anomaly Detection](#anomaly)<br>\n",
    "c) [Dimensionality Reduction and Clustering (t-SNE)](#clustering)<br>\n",
    "d) [Classifiers](#classifiers)<br>\n",
    "e) [A Deeper Look into Logistic Regression](#logistic)<br>\n",
    "f) [Oversampling with SMOTE](#smote)<br><br>\n",
    "\n",
    "IV. <b>Testing </b><br>\n",
    "a) [Testing with Logistic Regression](#testing_logistic)<br>\n",
    "b) [Neural Networks Testing (Undersampling vs Oversampling)](#neural_networks)\n",
    "\n",
    "<h2> 개요: </h2>\n",
    "I. <b>데이터 이해하기</b><br>\n",
    "a) [데이터 의미 수집](#gather)<br><br>\n",
    "\n",
    "II. <b>전처리</b><br>\n",
    "a) [스케일링 및 분산](#distributing)<br>\n",
    "b) [데이터 분할](#splitting)<br><br>\n",
    "\n",
    "III. <b>임의 언더샘플링과 오버샘플링Random UnderSampling and Oversampling</b><br>\n",
    "a) [분산과 상관관계](#correlating)<br>\n",
    "b) [이상 탐지](#anomaly)<br>\n",
    "c) [차원 축소 및 클러스터링(t-SNE)](#clustering)<br>\n",
    "d) [분류기](#classifiers)<br>\n",
    "e) [로지스틱 회귀 분석 더 자세히 보기](#logistic)<br>\n",
    "f) [SMOTE로 오버샘플링](#smote)<br><br>\n",
    "\n",
    "IV. <b>테스트 </b><br>\n",
    "a) [로지스틱 회귀 분석을 사용한 검정](#testing_logistic)<br>\n",
    "b) [신경망 테스트(언더샘플링 vs 오버샘플링)](#neural_networks)\n",
    "\n",
    "<h2>Correcting Previous Mistakes from Imbalanced Datasets: </h2>\n",
    "<ul>\n",
    "<li> Never test on the oversampled or undersampled dataset.</li>\n",
    "<li>If we want to implement cross validation, remember to oversample or undersample your training data <b>during</b> cross-validation, not before! </li>\n",
    "<li> Don't use <b>accuracy score </b> as a metric with imbalanced datasets (will be usually high and misleading), instead use <b>f1-score, precision/recall score or confusion matrix </b></li>\n",
    "</ul>\n",
    "\n",
    "## 불균형 데이터 세트의 이전 실수 수정:\n",
    "- 오버샘플링 또는 언더 샘플링 데이터 세트에서는 절대 테스트하지 하세요.\n",
    "- 교차 검증을 구형하려면 교차 검증 중에 train 데이터를 과도하게 샘플링하거나 과소 샘플링하는 것을 잊지 마십시오. 이전에는 그렇지 않았습니다!\n",
    "- 불균형 데이터 세트 matrix로 정확한(Accuracy) 점수를 사용하지 마십시오(일반적으로 높고 오해의 소지가 있음). 대신 f1 점수, 정밀도/호출 점수 또는 혼동 행렬(confusion matrix)을 사용하세요.\n",
    "\n",
    "<h2> References: </h2>\n",
    "<ul> \n",
    "<li>Hands on Machine Learning with Scikit-Learn & TensorFlow by Aurélien Géron (O'Reilly). CopyRight 2017 Aurélien Géron  </li>\n",
    "<li><a src=\"https://www.youtube.com/watch?v=DQC_YE3I5ig&t=794s\" > Machine Learning - Over-& Undersampling - Python/ Scikit/ Scikit-Imblearn </a>by Coding-Maniac</li>\n",
    "<li><a src=\"https://www.kaggle.com/lane203j/auprc-5-fold-c-v-and-resampling-methods\"> auprc, 5-fold c-v, and resampling methods\n",
    "</a> by Jeremy Lane (Kaggle Notebook) </li>\n",
    "</ul>\n",
    "\n",
    "## 참고자료:\n",
    "- Aurelien Géron(O'Reilly)의 Scikit-Learn & TensorFlow를 사용한 머신 러닝 실습. 저작권 2017 Aurelien Géron\n",
    "- [기계 학습 - 오버&언더샘플링 - Python/Scikit/Scikit-Imblearn by Coding-Maniac](https://www.youtube.com/watch?v=DQC_YE3I5ig&t=794s)\n",
    "- [by Jeremy Lane (Kaggle Notebook)](https://www.kaggle.com/lane203j/auprc-5-fold-c-v-and-resampling-methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae8dd7f3-80a7-4db9-a132-823b0e48c041",
    "_uuid": "c999e5f1ac81513263d83883008f2844209e9e07"
   },
   "source": [
    "## Gather Sense of Our Data:\n",
    "<a id=\"gather\"></a>\n",
    "The first thing we must do is gather a <b> basic sense </b> of our data. Remember, except for the <b>transaction</b> and <b>amount</b> we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already.   \n",
    "\n",
    "## 데이터를 이해하고 파악하기\n",
    "<a id=\"gather\"></a>\n",
    "우리가 가장 먼저 해야 할 일은 우리의 데이터에 대한 기본적인 정보를 수집하는 것입니다. 거래와 금액을 제외하고는 (개인 정보 보호상의 이유로) 다른 열이 무엇인지 알 수 없습니다. 알고 있는 열을 제외하고 모두 스케일링 되어 있다는 것을 알 수 있습니다.\n",
    "\n",
    "\n",
    "<h3> Summary: </h3>\n",
    "<ul>\n",
    "<li>The transaction amount is relatively <b>small</b>. The mean of all the amounts made is approximately USD 88. </li>\n",
    "<li>There are no <b>\"Null\"</b> values, so we don't have to work on ways to replace values. </li>\n",
    "<li> Most of the transactions were <b>Non-Fraud</b> (99.83%) of the time, while <b>Fraud</b> transactions occurs (017%) of the time in the dataframe. </li>\n",
    "</ul>\n",
    "\n",
    "### 요약:\n",
    "- 이 데이터 세트에서 모든 거래 금액의 평균은 대략 88달러로 비교적 적은 금액으로 이루어져 있습니다.\n",
    "- \"Null\"값이 없으므로 값을 대체할 방법을 강구할 필요가 없습니다.\n",
    "- 이 데이터프레임에서는 대부분의 거래는 'Non-Fraud'(99.83%) 이며, 'Fraud'(0.17%) 로 나타납니다.\n",
    "\n",
    "<h3> Feature Technicalities: </h3>\n",
    "<ul>\n",
    "<li> <b>PCA Transformation: </b>  The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).</li>\n",
    "<li> <b>Scaling:</b> Keep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled or at least that is what we are assuming the people that develop the dataset did.)</li>\n",
    "</ul>\n",
    "\n",
    "### 독립변수 기술:\n",
    "- PCA 변환\n",
    "    - 데이터 설명에 따르면 모든 독립변수가 PCA 변환(차원 축소 기법)을 거쳤다고 합니다.(Time과 Amount는 제외)\n",
    "- 스케일링\n",
    "    - PCA 변환 기능을 구현하려면 사전에 스케일링 해야합니다. (이 경우에는 모든 V 독립변수가 스케일링 되었거나 적어도 데이터셋을 개발한 사람들이 그렇게 가정한다는 것입니다.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.918649e-15</td>\n",
       "      <td>5.682686e-16</td>\n",
       "      <td>-8.761736e-15</td>\n",
       "      <td>2.811118e-15</td>\n",
       "      <td>-1.552103e-15</td>\n",
       "      <td>2.040130e-15</td>\n",
       "      <td>-1.698953e-15</td>\n",
       "      <td>-1.893285e-16</td>\n",
       "      <td>-3.147640e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.473120e-16</td>\n",
       "      <td>8.042109e-16</td>\n",
       "      <td>5.282512e-16</td>\n",
       "      <td>4.456271e-15</td>\n",
       "      <td>1.426896e-15</td>\n",
       "      <td>1.701640e-15</td>\n",
       "      <td>-3.662252e-16</td>\n",
       "      <td>-1.217809e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.918649e-15  5.682686e-16 -8.761736e-15  2.811118e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552103e-15  2.040130e-15 -1.698953e-15 -1.893285e-16 -3.147640e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.473120e-16  8.042109e-16  5.282512e-16  4.456271e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.426896e-15  1.701640e-15 -3.662252e-16 -1.217809e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imported Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches # 도형으로 시각화 하는 것\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression # 로지스틱 회귀\n",
    "from sklearn.svm import SVC # 서포트벡터머신 \n",
    "from sklearn.neighbors import KNeighborsClassifier # K means 알고리즘\n",
    "from sklearn.tree import DecisionTreeClassifier # 의사결정트리\n",
    "from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 분류기\n",
    "import collections # 컨테이너 데이터형\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split # 학습, 테스트 데이터 분리\n",
    "from sklearn.pipeline import make_pipeline # sklearn 파이프라인\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline # 불균형 데이터 세트 다루기\n",
    "from imblearn.over_sampling import SMOTE # 오버샘플링\n",
    "from imblearn.under_sampling import NearMiss # 언더샘플링\n",
    "from imblearn.metrics import classification_report_imbalanced # \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"../../archive/data/신용카드사기탐지/creditcard.csv\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Null 값 확인하기 \n",
    "print(df.isnull().sum().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 독립변수 확인하기\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fraud 99.83% of the dataset\n",
      "Frauds 0.17% of the dataset\n"
     ]
    }
   ],
   "source": [
    "# 클래스가 심하게 기울어져 있어 이 문제를 나중에 해결해야합니다.\n",
    "print(f\"No Fraud {round(df['Class'].value_counts()[0]/len(df) * 100,2)}% of the dataset\")\n",
    "print(f\"Frauds {round(df['Class'].value_counts()[1]/len(df) * 100,2)}% of the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "558c9b60-3f52-4da5-92fa-9fc4acbdbb3a",
    "_uuid": "c2bb0945a312508e908386fc87adc227f0afe0e0"
   },
   "source": [
    "**Note:**  Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!\n",
    "\n",
    "**메모:** \n",
    "원본 데이터 세트가 얼마나 불균형적인지 주목하십시오! 대부분의 거래는 사기가 아닙니다. 이 데이터 프레임을 예측 모델 및 분석의 기반으로 사용하면 많은 오류가 발생할 수 있으며 대부분의 거래가 사기가 아니라고 \"assume\"하기 때문에 알고리즘이 과도하게 적합할 수 있습니다. 하지만 우리는 모델이 사기를 탐지하기 위해 특정한 패턴을 학습하도록 하는 것이 목적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#0101DF','#DF0101']\n",
    "\n",
    "sns.countplot(data = df, x = \"Class\", palette = colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/46266247/229357738-fab387ba-9b67-43f0-bd83-39789ad10e47.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c9973d0-83bd-4b09-860e-c1f507f88310",
    "_uuid": "6894af2afdbfd5cd670d00b66f10ae49f1cab421"
   },
   "source": [
    "**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future.\n",
    "\n",
    "**분포:**\n",
    "분포를 보면 이러한 형상이 얼마나 치우쳐 있는지 알 수 있고, 다른 형태의 추가 분포도 볼 수 있습니다. 분포의 왜곡을 줄이는 데 도움이 되는 기술이 있으며, 향후 이 노트북에 구현될 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = df['Amount'].values\n",
    "time_val = df['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title(\"Distribution of Transaction Amount\", fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/46266247/229357740-cf4e4fbb-69b5-4fe1-aebf-0490cec1efab.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "72fdda5e-7f82-488d-a433-6157d6180bb8",
    "_uuid": "c5d6781e61c0ee84e72d26e8465bfd98ef91f3b9"
   },
   "source": [
    "<h2> Scaling and Distributing </h2>\n",
    "<a id=\"distributing\"></a>\n",
    "In this phase of our kernel, we will first scale the columns comprise of <b>Time</b> and <b>Amount </b>. Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
    "\n",
    "## 스케일링과 분산\n",
    "\n",
    "컨널의 이 단계에서는 먼저 Time과 Amount로 구성된 열을 확장합니다. 시간과 양은 다른 열과 마찬가지로 스케일링 되어야 합니다. 다른 한편으로, 우리는 동일한 양의 Fraud 및 Non-Fraud 사례를 보유하기 위해 DataFrame의 하위 샘플을 생성해야 하며, 이는 우리의 알고리즘이 거래가 Fraud인지 여부를 결정하는 패턴을 더 잘 이해할 수 있도록 도와줍니다.\n",
    "\n",
    "<h3> What is a sub-Sample?</h3>\n",
    "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
    "\n",
    "### 하위 샘플은 무엇인가?\n",
    "\n",
    "이 시나리오에서 우리의 서브샘플은 Fraud 와 Non-Fraud의 비율이 50/50 인 데이터 프레임이 될 것입니다. 즉, 하위 샘플은 동일한 양의 Fraud와 Non-Fraud 를 하게 됩니다.\n",
    "\n",
    "\n",
    "<h3> Why do we create a sub-Sample?</h3>\n",
    "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n",
    "<ul>\n",
    "<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n",
    "<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n",
    "</ul>\n",
    "\n",
    "### 하위 샘플은 왜 만드는 것인가?\n",
    "\n",
    "시작 부분에서 원본 데이터 프레임의 불균형이 심각하다는 것을 알 수 있었습니다. 원래 데이터 프레임을 사용하면 다음과 같은 문제가 발생합니다.\n",
    "\n",
    "- 과적합\n",
    "    - 우리의 분류 모델은 대부분의 경우 사기가 없다고 가정할 것입니다! 우리가 모델에 대해 원하는 것은 사기가 발생했을 때 확실하게 하는 것입니다.\n",
    "- 잘못된 상관 관계\n",
    "    - \"V\" 독립변수가 무엇을 의미하는지는 알 수 없지만, 종속변수와 독립변수 간의 실제 상관 관계를 확인할 수 없는 불균형 데이터 프레임을 사용하여 이러한 각 기능이 결과(사기 또는 사이 없음)에 어떤 영향을 미치는지 이해하는 것이 유용할 것입니다.\n",
    "\n",
    "<h3>Summary: </h3> \n",
    "<ul>\n",
    "<li> <b>Scaled amount </b> and <b> scaled time </b> are the columns with scaled values. </li>\n",
    "<li> There are <b>492 cases </b> of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. </li>\n",
    "<li>We concat the 492 cases of fraud and non fraud, <b>creating a new sub-sample. </b></li>\n",
    "</ul>\n",
    "\n",
    "### 요약:\n",
    "- 스케일화된 \"Amount\" 와 스케일화된 \"Time\" 은 스케일화된 값을 갖는 열입니다.\n",
    "- 데이터 세트에는 492 건의 Fraud 사례가 있으므로 492 건의 Non-Fraud 사례를 무작위로 받아 새로운 하위 데이터 프레임을 만들 수 있습니다.\n",
    "- 우리는 492건의 Fraud 및 Non-Fraud 사례를 결합하여 새로운 하위 샘플을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 대부분이 이미 확장되었기 때문에 남은 열을 확장해야 합니다.(Amount and Time)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler는 특이치가 발생하기 쉽습니다.\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "df.drop(['Amount', 'Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       1.783274    -0.994983 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.269825    -0.994983  1.191857  0.266151  0.166480  0.448154   \n",
       "2       4.983721    -0.994972 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.418291    -0.994972 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.670579    -0.994960 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "# Amount and Time are Scaled!\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a59c8c8d-a4bc-4671-aa2f-9f959c142cde",
    "_uuid": "5119c4ea9e0b9031dbc5937b56323da224985024"
   },
   "source": [
    "### Splitting the Data (Original DataFrame)\n",
    "<a id=\"splitting\"></a>\n",
    "Before proceeding with the <b> Random UnderSampling technique</b> we have to separate the orginal dataframe. <b> Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques.</b> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.  \n",
    "\n",
    "### 데이터 분리 ( 원본 데이터 프레임 )\n",
    "랜덤 언더샘플링 기법을 진행하기 전에 원본 데이터 프레임을 분리해야 합니다. 왜 그럴까요?   \n",
    "테스트 목적으로 랜덤 언더샘플링 또는 오버샘플링 기법을 구현할 때 데이터를 분할하지만 이러한 기법으로 생성된 테스트 세트가 아닌 원래 테스트 세트에서 모델을 테스트하고자 합니다.   \n",
    "주요 목표는 모델을 언더샘플 및 오버샘플 데이터 프레임(모델이 패턴을 감지할 수 있도록)에 맞추고 원래 테스트 세트에서 테스트하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83% of the dataset\n",
      "Frauds 0.17% of the dataset\n",
      "Train: [ 30473  30496  31002 ... 284804 284805 284806] Test: [    0     1     2 ... 57017 57018 57019]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [ 30473  30496  31002 ... 113964 113965 113966]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [ 81609  82400  83053 ... 170946 170947 170948]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [150654 150660 150661 ... 227866 227867 227868]\n",
      "Train: [     0      1      2 ... 227866 227867 227868] Test: [212516 212644 213092 ... 284804 284805 284806]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Label Distributions: \n",
      "\n",
      "[0.99827076 0.00172924]\n",
      "[0.99827952 0.00172048]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "print(f\"No Frauds {round(df['Class'].value_counts()[0]/len(df) * 100,2)}% of the dataset\")\n",
    "print(f\"Frauds {round(df['Class'].value_counts()[1]/len(df) * 100, 2)}% of the dataset\")\n",
    "\n",
    "X = df.drop('Class', axis = 1)\n",
    "y = df['Class']\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X,y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# 우리는 이미 하위 샘플 데이터를 위한 X_train과 y_train을 가지고 있기 때문에 이 변수들을 구별하고 덮어쓰지 않기 위해 원본을 사용하고 있습니다.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest\n",
    "\n",
    "# 레이블 분포 확인\n",
    "\n",
    "# 배열로 바꾼다.\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# 학습, 테스트 라벨 분포가 유사한지 확인\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-'*100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/len(original_ytrain))\n",
    "print(test_counts_label/len(original_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "956d34b9-e562-4b70-a2f8-fbe060273a83",
    "_uuid": "cc554c4ffec656cb38d01c034f2cd338e1cb4565"
   },
   "source": [
    "## Random Under-Sampling:\n",
    "\n",
    "In this phase of the project we will implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset </b> and thus avoiding our models to overfitting.\n",
    "\n",
    "프로젝트의 이 단계에서 우리는 임의 하위 샘플링을 구현할 것입니다. 이는 기본적으로 데이터를 제거하여 보다 균형 잡힌 데이터 세트를 보유하고 모델이 과적합되는 것을 방지하는 것으로 구성됩니다.\n",
    "\n",
    "[랜덤 샘플링 vs 오버샘플링](https://hwi-doc.tistory.com/entry/%EC%96%B8%EB%8D%94-%EC%83%98%ED%94%8C%EB%A7%81Undersampling%EA%B3%BC-%EC%98%A4%EB%B2%84-%EC%83%98%ED%94%8C%EB%A7%81Oversampling)\n",
    "\n",
    "#### Steps:\n",
    "<ul>\n",
    "<li>The first thing we have to do is determine how <b>imbalanced</b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  </li>\n",
    "<li>Once we determine how many instances are considered <b>fraud transactions </b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions</b> to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  </li>\n",
    "<li> After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data</b> to see if our models can maintain a certain accuracy everytime we run this script.</li>\n",
    "</ul>\n",
    "\n",
    "#### 단계:\n",
    "- 첫번째로 클래스가 얼마나 불균형인지를 결정하는 것입니다. ( 클래스 열에서 \"value_counts()\" 를 사용하여 각 레이블의 양을 결정 )\n",
    "- \"Fraud\"로 간주되는 사례가 얼마나 되는지를 파악하면 ( Fraud = '1' ), \"Non-Fraud\"를 \"Fraud\"와 동일한 양(50/50 비율을 원한다고 가정하면), 이는 \"Fraud\" 492건, \"Non-Fraud\" 492건에 해당합니다.\n",
    "- 이 기술을 구현한 후, 클래스와 관련하여 50/50 비율의 데이터 프레임의 하위 샘픙을 갖게 됩니다. 그런 다음 구현할 다음 단계는 이 스크립트를 실행할 때마다 모델이 일정한 정확도를 유지할 수 있는지 확인하기 위해 데이터를 섞는 것입니다.\n",
    "\n",
    "\n",
    "**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss</b> (bringing 492 non-fraud transaction  from 284,315 non-fraud transaction)\n",
    "\n",
    "    \n",
    "**메모:**\n",
    "- 임의 언더샘플링의 주요 문제는 엄청난 양의 정보 손실(284,315개의 \"Non-Fraud\" 거래에서 492개의 거래 발생)이 있기 때문에 분류 모델이 원하는 만큼 정확하게 수행되지 않을 위험이 있다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121387</th>\n",
       "      <td>4.373646</td>\n",
       "      <td>-0.100072</td>\n",
       "      <td>0.665682</td>\n",
       "      <td>-1.130127</td>\n",
       "      <td>-0.354074</td>\n",
       "      <td>0.350375</td>\n",
       "      <td>-0.888343</td>\n",
       "      <td>-1.064931</td>\n",
       "      <td>0.520398</td>\n",
       "      <td>-0.290256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485130</td>\n",
       "      <td>0.093709</td>\n",
       "      <td>-0.404079</td>\n",
       "      <td>-0.359898</td>\n",
       "      <td>0.464065</td>\n",
       "      <td>0.371279</td>\n",
       "      <td>1.091993</td>\n",
       "      <td>-0.168559</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218442</th>\n",
       "      <td>2.426605</td>\n",
       "      <td>0.665280</td>\n",
       "      <td>-6.352337</td>\n",
       "      <td>-2.370335</td>\n",
       "      <td>-4.875397</td>\n",
       "      <td>2.335045</td>\n",
       "      <td>-0.809555</td>\n",
       "      <td>-0.413647</td>\n",
       "      <td>-4.082308</td>\n",
       "      <td>2.239089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186898</td>\n",
       "      <td>1.325218</td>\n",
       "      <td>1.226745</td>\n",
       "      <td>-1.485217</td>\n",
       "      <td>-1.470732</td>\n",
       "      <td>-0.240053</td>\n",
       "      <td>0.112972</td>\n",
       "      <td>0.910591</td>\n",
       "      <td>-0.650944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48645</th>\n",
       "      <td>1.215678</td>\n",
       "      <td>-0.481314</td>\n",
       "      <td>-1.039470</td>\n",
       "      <td>0.605510</td>\n",
       "      <td>1.775681</td>\n",
       "      <td>1.302934</td>\n",
       "      <td>-0.056428</td>\n",
       "      <td>0.492817</td>\n",
       "      <td>0.874186</td>\n",
       "      <td>0.125303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>0.019832</td>\n",
       "      <td>0.254695</td>\n",
       "      <td>-0.047067</td>\n",
       "      <td>0.249531</td>\n",
       "      <td>0.442921</td>\n",
       "      <td>-0.164943</td>\n",
       "      <td>0.025734</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6338</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.906272</td>\n",
       "      <td>0.316459</td>\n",
       "      <td>3.809076</td>\n",
       "      <td>-5.615159</td>\n",
       "      <td>6.047445</td>\n",
       "      <td>1.554026</td>\n",
       "      <td>-2.651353</td>\n",
       "      <td>-0.746579</td>\n",
       "      <td>0.055586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>0.208828</td>\n",
       "      <td>-0.511747</td>\n",
       "      <td>-0.583813</td>\n",
       "      <td>-0.219845</td>\n",
       "      <td>1.474753</td>\n",
       "      <td>0.491192</td>\n",
       "      <td>0.518868</td>\n",
       "      <td>0.402528</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31002</th>\n",
       "      <td>1.253406</td>\n",
       "      <td>-0.570049</td>\n",
       "      <td>-5.685013</td>\n",
       "      <td>5.776516</td>\n",
       "      <td>-7.064977</td>\n",
       "      <td>5.902715</td>\n",
       "      <td>-4.715564</td>\n",
       "      <td>-1.755633</td>\n",
       "      <td>-6.958679</td>\n",
       "      <td>3.877795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299489</td>\n",
       "      <td>1.128641</td>\n",
       "      <td>-0.962960</td>\n",
       "      <td>-0.110045</td>\n",
       "      <td>-0.177733</td>\n",
       "      <td>-0.089175</td>\n",
       "      <td>-0.049447</td>\n",
       "      <td>0.303445</td>\n",
       "      <td>0.219380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "121387       4.373646    -0.100072  0.665682 -1.130127 -0.354074  0.350375   \n",
       "218442       2.426605     0.665280 -6.352337 -2.370335 -4.875397  2.335045   \n",
       "48645        1.215678    -0.481314 -1.039470  0.605510  1.775681  1.302934   \n",
       "6338        -0.293440    -0.906272  0.316459  3.809076 -5.615159  6.047445   \n",
       "31002        1.253406    -0.570049 -5.685013  5.776516 -7.064977  5.902715   \n",
       "\n",
       "              V5        V6        V7        V8  ...       V20       V21  \\\n",
       "121387 -0.888343 -1.064931  0.520398 -0.290256  ...  0.485130  0.093709   \n",
       "218442 -0.809555 -0.413647 -4.082308  2.239089  ...  0.186898  1.325218   \n",
       "48645  -0.056428  0.492817  0.874186  0.125303  ...  0.012786  0.019832   \n",
       "6338    1.554026 -2.651353 -0.746579  0.055586  ...  0.388307  0.208828   \n",
       "31002  -4.715564 -1.755633 -6.958679  3.877795  ...  0.299489  1.128641   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "121387 -0.404079 -0.359898  0.464065  0.371279  1.091993 -0.168559  0.046432   \n",
       "218442  1.226745 -1.485217 -1.470732 -0.240053  0.112972  0.910591 -0.650944   \n",
       "48645   0.254695 -0.047067  0.249531  0.442921 -0.164943  0.025734  0.038966   \n",
       "6338   -0.511747 -0.583813 -0.219845  1.474753  0.491192  0.518868  0.402528   \n",
       "31002  -0.962960 -0.110045 -0.177733 -0.089175 -0.049447  0.303445  0.219380   \n",
       "\n",
       "        Class  \n",
       "121387      0  \n",
       "218442      1  \n",
       "48645       0  \n",
       "6338        1  \n",
       "31002       1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 클래스가 심하게 치우쳐 있으므로 클래스의 정규 분포를 얻으려면 클래스를 동등하게 만들어야 합니다.\n",
    "\n",
    "# 하위 샘플을 만들기 전에 데이터를 섞습니다.\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# 492 건의 Fraud와 Non-Fraud의 클래스 정의\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492] # Non-fraud의 데이터의 개수는 많습니다. 그 중 fraud의 데이터의 개수와 같은 492 개의 데이터만 인덱싱해줍니다.\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df,non_fraud_df]) # 위 두개의 데이터 프레임을 결합하여 fraud와 non-fraud의 데이터 비율이 비슷하게 분포한 데이터 프레임을 만들어 줍니다.\n",
    "\n",
    "# 데이터 행의 순서를 섞는다.\n",
    "new_df = normal_distributed_df.sample(frac=1,random_state=42)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77198464-c0f8-4694-ac0b-4b29b94d0da3",
    "_uuid": "b6818122806657e7accb8be1f4bf17086bb9b149"
   },
   "source": [
    "##  Equally Distributing and Correlating: \n",
    "<a id=\"correlating\"></a>\n",
    "Now that we have our dataframe correctly balanced, we can go further with our <b>analysis</b> and <b>data preprocessing</b>.\n",
    "\n",
    "## 균등 분산과 상관관계\n",
    "이제 데이터프레임의 균형이 올바르게 조정되었으므로 분석과 데이터 전처리를 더 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the Classes in the subsample dataset\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of the Classes in the subsample dataset\")\n",
    "print(new_df['Class'].value_counts()/len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=new_df, x=\"Class\", palette=colors)\n",
    "plt.title(\"Equally Distributed Classed\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/46266247/229357741-6b3ea131-0d26-4088-bf25-5b071902a5f3.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0abc31ee-a78e-43af-822f-f06772d00c1c",
    "_uuid": "88477bac6687f110e9d64ec22837c250d85d2a2b"
   },
   "source": [
    "<h3> Correlation Matrices </h3>\n",
    "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n",
    "\n",
    "\n",
    "### 상관 행렬\n",
    "- 상관 행렬은 데이터를 이행하는 데 필수적인 요소입니다. 특정 거래가 사기인지 여부에 큰 영향을 미치는 독립 변수가 있는지 알고 싶습니다. 그러나 \"Fraud\" 거래와 관련하여 어떤 독립변수가 높은 양 또는 음의 상관관계를 가지는지 확인하려면 올바른 데이터 프레임(하위 샘플)을 사용하는 것이 중요합니다.\n",
    "\n",
    "### Summary and Explanation: \n",
    "<ul>\n",
    "<li><b>Negative Correlations: </b>V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  </li>\n",
    "<li> <b> Positive Correlations: </b> V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. </li>\n",
    "<li> <b>BoxPlots: </b>  We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions. </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "### 요약 및 설명:\n",
    "- 음의 상관관계 : V17, V14, V12, V10은 음의 상관관계가 있습니다. 이러한 값이 낮을수록 최종 결과는 \"Fraud\" 거래가 될 가능성이 높습니다.\n",
    "- 양의 상관관계 : V2, V4, V11, V19는 양의 상관관계가 있습니다. 이러한 값이 높을수록 최종 결과가 \"Fraud\" 거래로 이러질 가능성이 높아집니다.\n",
    "- 상자 그림(Boxplot) : 상자 그림을 사용하여 \"Fraud\" 및 \"Non-Fraud\" 거래에서 이러한 독립변수의 분포를 더 잘 이해할 것입니다.\n",
    "\n",
    "**Note:** We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe.\n",
    "\n",
    "**메모:**\n",
    "- 상관 행렬에는 하위 샘플을 사용해야 합니다. 그렇지 않으면 상관 행렬은 클래스 간의 높은 불균형의 영향을 받을 것입니다. 이 문제는 원래 데이터 프레임의 높은 클래스 불균형 때문에 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관관계에서는 하위 샘플을 이용해야 합니다.\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "# 전체 데이터프레임\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize = 14)\n",
    "\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={\"size\":20}, ax=ax2)\n",
    "ax2.set_title(\"SubSample Correlation Matrix \\n (use for reference)\", fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/46266247/229357742-c9f3c001-ff83-4813-9cd1-4403da33dca7.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# 클래스와의 부정적인 상관 관계 ( 독립 변수 값이 낮을수록 \"Fraud\" 거래일 가능성이 높음 )\n",
    "\n",
    "sns.boxplot(x='Class', y = 'V17', data=new_df, palette=colors, ax=axes[0])\n",
    "axes[0].set_title('V17 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x='Class', y = 'V14', data = new_df, palette= colors, ax=axes[1])\n",
    "axes[1].set_title('V14 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x='Class', y = 'V12', data = new_df, palette=colors, ax=axes[2])\n",
    "axes[2].set_title('V12 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x='Class', y = 'V10', data = new_df, palette= colors, ax = axes[3])\n",
    "axes[3].set_title('V10 vs Class Negative Correlation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/46266247/229357744-9585f4ed-adb6-48e9-b26c-83d1ad6c6f38.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# 양의 상관관계 ( 독립변수 값이 높을수록 \"Fraud\" 거래일 확률이 증가합니다. )\n",
    "sns.boxplot(x='Class', y='V11', data= new_df, palette=colors, ax=axes[0])\n",
    "axes[0].set_title('V11 vs Class Positive Correlation')\n",
    "\n",
    "sns.boxplot(x='Class', y= 'V4', data=new_df, palette=colors, ax=axes[1])\n",
    "axes[1].set_title('V4 vs Class Positive Correlation')\n",
    "\n",
    "sns.boxplot(x='Class', y='V2', data=new_df, palette=colors, ax=axes[2])\n",
    "axes[2].set_title('V2 vs Class Positive Correlation')\n",
    "\n",
    "sns.boxplot(x='Class', y='V19', data=new_df, palette=colors, ax=axes[3])\n",
    "axes[3].set_title('V19 vs Class Positive Correlation')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/46266247/229357745-375b625d-848f-4416-a546-41b82486e2e1.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93e56c89-185e-40d2-9ccc-29b123feb5a6",
    "_uuid": "a721282c0f44ec8030bbad6d0220091bde8cbec8"
   },
   "source": [
    "## Anomaly Detection:\n",
    "\n",
    "Our main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models.\n",
    "\n",
    "## 이상치 탐지:\n",
    "\n",
    "이 센션의 주요 목표는 클래스와 상관관계가 높은 독립변수에서 '극단적인 이상값'을 제거하는 것입니다. 이렇게 하면 모델의 정확도에 긍정적인 영향을 미칩니다.\n",
    "\n",
    "\n",
    "### Interquartile Range Method:\n",
    "<ul>\n",
    "<li> <b>Interquartile Range (IQR): </b> We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.  </li>\n",
    "<li> <b>Boxplots: </b> Besides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme). </li>\n",
    "</ul>\n",
    "\n",
    "### 사 분위간 범위 방법: <ul> <li> <b>사 분위간 범위(IQR): </b> 75번째 백분위수와 25번째 백분위수의 차이로 계산합니다. 우리의 목표는 75번째 백분위수와 25번째 백분위수를 넘어서는 임계값을 생성하여 일부 인스턴스가 이 임계값을 통과할 경우 인스턴스가 삭제되도록 하는 것입니다.  </li> <li> <b>박스플롯: </b> 25번째 및 75번째 백분위수(사각형의 양쪽 끝)를 쉽게 볼 수 있을 뿐만 아니라 극단값 이상값(극단의 하한과 상한을 넘는 지점)도 쉽게 볼 수 있습니다. </li> </ul>\n",
    "\n",
    "### 사분위간 범위 방법:\n",
    "- 사분위간 범위(IQR):\n",
    "    - 75번째 백분위수와 25번째 백분위수의 차이로 계산합니다.\n",
    "    - 목표는 75번째 백분위수와 25번째 백분위수를 넘어서는 임계값을 생성하여 일부 인스턴스가 이 임계값을 통과할 경우 인스턴스가 삭제되도록 하는 것입니다.\n",
    "- 상자 그림:\n",
    "    - 25번째 및 75번째 백분위수(사각형의 양쪽 끝)를 쉽게 볼 수 있을 뿐만 아니라 극단값 이상값(극단의 하한과 상한을 넘는 지점)도 쉽게 볼 수 있습니다.\n",
    "\n",
    "### Outlier Removal Tradeoff:\n",
    "We have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.  <br><br>\n",
    "\n",
    "### 이상값 제거 균형 맞추기:\n",
    "이상값을 제거하기 위한 임계값을 어디까지 설정할지 신중하게 결정해야 합니다.  \n",
    "임계값은 숫자(예:1.5)에 사분위수 범위(IQR)를 곱하여 결정합니다. 이 임계값이 높을수록 더 적은 수의 이상값을 감지하고 낮을수록 더 많은 이상값을 감지합니다.\n",
    "\n",
    "**The Tradeoff: **\n",
    "The lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models.\n",
    "\n",
    "**The Tradeoff:**  \n",
    "임계값이 낮을수록 더 많은 이상값을 제거할 수 있지만, 단순한 이상값보다는 '극단적인 이상값'에 더 초점을 맞추고자 합니다.   \n",
    "그 이유는 정보 손실의 위험이 있어 모델의 정확도가 낮아질 수 있기 때문입니다. 이 임계값을 조절하면서 분류 모델의 정확도에 어떤 영향을 미치는지 확인할 수 있습니다.\n",
    "\n",
    "\n",
    "### Summary:\n",
    "<ul>\n",
    "<li> <b> Visualize Distributions: </b> We first start by visualizing the distribution of the feature we are going to use to eliminate some of the outliers. V14 is the only feature that has a Gaussian distribution compared to features V12 and V10. </li>\n",
    "<li><b>Determining the threshold: </b> After we decide which number we will use to multiply with the iqr (the lower more outliers removed), we will proceed in determining the upper and lower thresholds by substrating q25 - threshold (lower extreme threshold) and adding q75 + threshold (upper extreme threshold). </li>\n",
    "<li> <b>Conditional Dropping: </b> Lastly, we create a conditional dropping stating that if the \"threshold\" is exceeded in both extremes, the instances will be removed. </li>\n",
    "<li> <b> Boxplot Representation: </b> Visualize through the boxplot that the number of \"extreme outliers\" have been reduced to a considerable amount. </li>\n",
    "</ul>\n",
    "\n",
    "### 요약: <ul> <li> <b> 분포 시각화: </b> 먼저 일부 이상값을 제거하는 데 사용할 특징의 분포를 시각화하는 것으로 시작합니다. V14는 특징 V12 및 V10에 비해 가우스 분포를 갖는 유일한 특징입니다. </li> <li><b>임계값을 결정합니다: </b> iqr(더 많은 이상값을 제거할수록 낮은 값)을 곱하는 데 사용할 숫자를 결정한 후, q25 - 임계값(하한 임계값)을 기판화하고 q75 + 임계값(상한 임계값)을 더하여 상한 및 하한 임계값을 결정할 것입니다. </li> <li> <b>조건부 삭제: </b> 마지막으로, 양쪽 극단 모두에서 \"임계값\"을 초과하면 인스턴스를 제거한다는 조건부 삭제가 생성됩니다. </li> <li> <b> 박스 플롯 표현: </b> 박스 플롯을 통해 \"극단값 이상값\"의 수가 상당량 감소했음을 시각화합니다. </li> </ul>\n",
    "\n",
    "### 요약:\n",
    "- 분포 시각화(Visualize Distributions):\n",
    "    - 일부 이상값을 제거하는 데 사용할 특징의 분포를 시각화하는 것으로 시작합니다.\n",
    "    - V14는 독립변수 V12 및 V10에 비해 가우스 분포를 갖는 유일한 특징입니다.\n",
    "- 임계값 결정(Determining the threshold):\n",
    "    - IQR(더 많은 이상\n",
    "\n",
    "**Note:** After implementing outlier reduction our accuracy has been improved by over 3%! Some outliers can distort the accuracy of our models but remember, we have to avoid an extreme amount of information loss or else our model runs the risk of underfitting.\n",
    "\n",
    "\n",
    "**Reference**: More information on Interquartile Range Method: <a src=\"https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/\"> How to Use Statistics to Identify Outliers in Data </a> by Jason Brownless (Machine Learning Mastery blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9c690dfa-8fed-44e5-99f5-ff4eb6f87f16",
    "_kg_hide-input": true,
    "_uuid": "b6963900379db5b0d4adf92f8c7f959164e9119f"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "\n",
    "v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\n",
    "ax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
    "ax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "\n",
    "v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\n",
    "ax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2e19fe33-f85a-4ffd-8e4a-807d0e0fb992",
    "_kg_hide-input": true,
    "_uuid": "21e43406e62a9561fba2f065ce15a8d87a1bf389"
   },
   "outputs": [],
   "source": [
    "# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\n",
    "v14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
    "print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n",
    "v14_iqr = q75 - q25\n",
    "print('iqr: {}'.format(v14_iqr))\n",
    "\n",
    "v14_cut_off = v14_iqr * 1.5\n",
    "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
    "print('Cut Off: {}'.format(v14_cut_off))\n",
    "print('V14 Lower: {}'.format(v14_lower))\n",
    "print('V14 Upper: {}'.format(v14_upper))\n",
    "\n",
    "outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
    "print('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
    "print('V10 outliers:{}'.format(outliers))\n",
    "\n",
    "new_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n",
    "print('----' * 44)\n",
    "\n",
    "# -----> V12 removing outliers from fraud transactions\n",
    "v12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n",
    "v12_iqr = q75 - q25\n",
    "\n",
    "v12_cut_off = v12_iqr * 1.5\n",
    "v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\n",
    "print('V12 Lower: {}'.format(v12_lower))\n",
    "print('V12 Upper: {}'.format(v12_upper))\n",
    "outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\n",
    "print('V12 outliers: {}'.format(outliers))\n",
    "print('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
    "new_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\n",
    "print('Number of Instances after outliers removal: {}'.format(len(new_df)))\n",
    "print('----' * 44)\n",
    "\n",
    "\n",
    "# Removing outliers V10 Feature\n",
    "v10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n",
    "v10_iqr = q75 - q25\n",
    "\n",
    "v10_cut_off = v10_iqr * 1.5\n",
    "v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\n",
    "print('V10 Lower: {}'.format(v10_lower))\n",
    "print('V10 Upper: {}'.format(v10_upper))\n",
    "outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n",
    "print('V10 outliers: {}'.format(outliers))\n",
    "print('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
    "new_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n",
    "print('Number of Instances after outliers removal: {}'.format(len(new_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "66e44398-7c91-4cce-9778-4512cb838973",
    "_kg_hide-input": true,
    "_uuid": "ac80d9cfb07f1865094a8d460ae801750e93d694"
   },
   "outputs": [],
   "source": [
    "f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n",
    "\n",
    "colors = ['#B3F9C5', '#f9c5b3']\n",
    "# Boxplots with outliers removed\n",
    "# Feature V14\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\n",
    "ax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n",
    "            arrowprops=dict(facecolor='black'),\n",
    "            fontsize=14)\n",
    "\n",
    "# Feature 12\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\n",
    "ax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n",
    "            arrowprops=dict(facecolor='black'),\n",
    "            fontsize=14)\n",
    "\n",
    "# Feature V10\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\n",
    "ax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n",
    "            arrowprops=dict(facecolor='black'),\n",
    "            fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74903f3b-dc6b-40ba-abc8-86c3df5ca46e",
    "_uuid": "0b365b10bd363c23068accc448509ced879f1670"
   },
   "source": [
    "<h2>Dimensionality Reduction and Clustering: </h2>\n",
    "<a id=\"clustering\"></a>\n",
    "\n",
    "<h3>Understanding t-SNE:  </h3>\n",
    "In order to understand this algorithm you have to understand the following terms: <br>\n",
    "<ul>\n",
    "<li> <b> Euclidean Distance </b></li>\n",
    "<li> <b>Conditional Probability</b> </li>\n",
    "<li><b>Normal and T-Distribution Plots</b> </li>\n",
    "</ul> \n",
    "\n",
    "**Note:** If you want a simple instructive video look at <a href=\"https://www.youtube.com/watch?v=NEaUSP4YerM\"> StatQuest: t-SNE, Clearly Explained </a> by Joshua Starmer\n",
    "\n",
    "\n",
    "<h3> Summary: </h3>\n",
    "<ul> \n",
    "<li>t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset. </li>\n",
    "<li> Although the subsample is pretty small, the t-SNE algorithm is able to detect clusters pretty accurately in every scenario (I shuffle the dataset before running t-SNE)</li>\n",
    "<li> This gives us an indication that further predictive models will perform pretty well in separating fraud cases from non-fraud cases. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f83cde6b-90d0-4e9d-ac63-fb69780431b2",
    "_kg_hide-input": true,
    "_uuid": "af3027e7df67b75c92c88d597003632e285c9bff"
   },
   "outputs": [],
   "source": [
    "# New_df is from the random undersample data (fewer instances)\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "\n",
    "# T-SNE Implementation\n",
    "t0 = time.time()\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
    "t1 = time.time()\n",
    "print(\"T-SNE took {:.2} s\".format(t1 - t0))\n",
    "\n",
    "# PCA Implementation\n",
    "t0 = time.time()\n",
    "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
    "t1 = time.time()\n",
    "print(\"PCA took {:.2} s\".format(t1 - t0))\n",
    "\n",
    "# TruncatedSVD\n",
    "t0 = time.time()\n",
    "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n",
    "t1 = time.time()\n",
    "print(\"Truncated SVD took {:.2} s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "07015ae5-f7ac-4d64-8f41-1e4b7c9dd2ac",
    "_kg_hide-input": true,
    "_uuid": "084f2a7421c2212082491d2a90e65d65c52b434a"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n",
    "# labels = ['No Fraud', 'Fraud']\n",
    "f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n",
    "\n",
    "\n",
    "blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n",
    "red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n",
    "\n",
    "\n",
    "# t-SNE scatter plot\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax1.set_title('t-SNE', fontsize=14)\n",
    "\n",
    "ax1.grid(True)\n",
    "\n",
    "ax1.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "\n",
    "# PCA scatter plot\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax2.set_title('PCA', fontsize=14)\n",
    "\n",
    "ax2.grid(True)\n",
    "\n",
    "ax2.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# TruncatedSVD scatter plot\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax3.set_title('Truncated SVD', fontsize=14)\n",
    "\n",
    "ax3.grid(True)\n",
    "\n",
    "ax3.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb2c480a-090a-4cfb-b12e-3b74c325826c",
    "_uuid": "1b63bfd92008043cc1a336f924c835e73792f6d8"
   },
   "source": [
    "<h2> Classifiers (UnderSampling):  </h2>\n",
    "<a id=\"classifiers\"></a>\n",
    "In this section we will train four types of classifiers and decide which classifier will be more effective in detecting <b>fraud transactions</b>.  Before we have to split our data into training and testing sets and separate the features from the labels.\n",
    "\n",
    "## Summary: \n",
    "<ul>\n",
    "<li> <b> Logistic Regression </b> classifier is more accurate than the other three classifiers in most cases. (We will further analyze Logistic Regression) </li>\n",
    "<li><b> GridSearchCV </b> is used to determine the paremeters that gives the best predictive score for the classifiers. </li>\n",
    "<li> Logistic Regression has the best Receiving Operating Characteristic score  (ROC), meaning that LogisticRegression pretty accurately separates <b> fraud </b> and <b> non-fraud </b> transactions.</li>\n",
    "</ul>\n",
    "\n",
    "## Learning Curves:\n",
    "<ul>\n",
    "<li>The <b>wider the  gap</b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)</b>.</li>\n",
    "<li> If the score is low in both training and cross-validation sets</b> this is an indication that our model is <b>underfitting (high bias)</b></li>\n",
    "<li><b> Logistic Regression Classifier</b>  shows the best score in both training and cross-validating sets.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "85ce8738-7599-4b06-a722-5c0ed073599b",
    "_kg_hide-input": true,
    "_uuid": "e3751d88766a982119e522e27a9c0c647f20af85"
   },
   "outputs": [],
   "source": [
    "# Undersampling before cross validating (prone to overfit)\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "288a65b7-8b86-44b1-973d-38dbcfe82bbb",
    "_kg_hide-input": true,
    "_uuid": "fb0a479efaa7147d6702c2c24083f1118621863f"
   },
   "outputs": [],
   "source": [
    "# Our data is already scaled we should split our training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This is explicitly used for undersampling.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bccd5685-a979-451e-85b3-1cb968523540",
    "_kg_hide-input": true,
    "_uuid": "28f5178089d2d133b9e7478c1c7dc7a1f98aabee"
   },
   "outputs": [],
   "source": [
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7810d0b9-b4e5-4b7f-909b-c127365b167c",
    "_kg_hide-input": true,
    "_uuid": "8dd4ea07fd60973fccabc2d46af28a09b0de9178"
   },
   "outputs": [],
   "source": [
    "# Let's implement simple classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb37c0f6-9cfe-48b6-92d3-475d5e6767a6",
    "_kg_hide-input": true,
    "_uuid": "fe129af379caccc5428cf1836e6c96bd32e68feb"
   },
   "outputs": [],
   "source": [
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a1c35773-f4c7-4caf-9911-532784c9eae0",
    "_kg_hide-input": true,
    "_uuid": "d15b1ab16737358806e34c48dc57aa238cf0cfd2"
   },
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best parameters.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n",
    "grid_knears.fit(X_train, y_train)\n",
    "# KNears best estimator\n",
    "knears_neighbors = grid_knears.best_estimator_\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "grid_svc = GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "\n",
    "# SVC best estimator\n",
    "svc = grid_svc.best_estimator_\n",
    "\n",
    "# DecisionTree Classifier\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "              \"min_samples_leaf\": list(range(5,7,1))}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "\n",
    "# tree best estimator\n",
    "tree_clf = grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f327bcd-335f-4e49-af07-fc4214dbcbdc",
    "_kg_hide-input": true,
    "_uuid": "1b2108bf377b924ed8a6efe580d9e162a132cd9e"
   },
   "outputs": [],
   "source": [
    "# Overfitting Case\n",
    "\n",
    "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "\n",
    "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
    "print('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
    "print('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
    "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38e430ef-0160-47a1-9b6f-11ff62c5ecc0",
    "_kg_hide-input": true,
    "_uuid": "eeb5736b279bb8fa3804689a175394f216ec4f72"
   },
   "outputs": [],
   "source": [
    "# We will undersample during cross validating\n",
    "undersample_X = df.drop('Class', axis=1)\n",
    "undersample_y = df['Class']\n",
    "\n",
    "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n",
    "    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n",
    "    \n",
    "undersample_Xtrain = undersample_Xtrain.values\n",
    "undersample_Xtest = undersample_Xtest.values\n",
    "undersample_ytrain = undersample_ytrain.values\n",
    "undersample_ytest = undersample_ytest.values \n",
    "\n",
    "undersample_accuracy = []\n",
    "undersample_precision = []\n",
    "undersample_recall = []\n",
    "undersample_f1 = []\n",
    "undersample_auc = []\n",
    "\n",
    "# Implementing NearMiss Technique \n",
    "# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\n",
    "print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n",
    "# Cross Validating the right way\n",
    "\n",
    "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
    "    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
    "    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n",
    "    \n",
    "    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bb72803c-3ea3-40cd-8ac3-399540ab7f5a",
    "_kg_hide-input": true,
    "_uuid": "a12fb2f7e104931bb78e1bd6cfc5a516c970708b"
   },
   "outputs": [],
   "source": [
    "# Let's Plot LogisticRegression Learning Curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    # First Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
    "    ax1.set_xlabel('Training size (m)')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc=\"best\")\n",
    "    \n",
    "    # Second Estimator \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n",
    "    ax2.set_xlabel('Training size (m)')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend(loc=\"best\")\n",
    "    \n",
    "    # Third Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n",
    "    ax3.set_xlabel('Training size (m)')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend(loc=\"best\")\n",
    "    \n",
    "    # Fourth Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n",
    "    ax4.set_xlabel('Training size (m)')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.grid(True)\n",
    "    ax4.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b8302aa-0207-455f-8c1a-78ff3e9b5141",
    "_kg_hide-input": true,
    "_uuid": "15b262baa0c61c288a5453031b4d7f80f5a7a5ab"
   },
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "plot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "780e485a-ea64-48a0-ad97-a7516b047f32",
    "_kg_hide-input": true,
    "_uuid": "fdd59bf2c7a8e61cfb401142570643e8a29cf86b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "# Create a DataFrame with all the scores and the classifiers names.\n",
    "\n",
    "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
    "\n",
    "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "57c211c6-e88f-4634-b321-4949df08815d",
    "_kg_hide-input": true,
    "_uuid": "cb2e4715e91e36f2029ef2a5c241991ff162cd9f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
    "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
    "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
    "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89b0b9b6-ef82-4b69-9517-e89a79696dbb",
    "_kg_hide-input": true,
    "_uuid": "9d57aad23f3f72f3c45bf80b089a65acbce2a9ab"
   },
   "outputs": [],
   "source": [
    "log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n",
    "knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\n",
    "svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\n",
    "tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n",
    "\n",
    "\n",
    "def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n",
    "    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n",
    "    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n",
    "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "    \n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f56e6936-314c-42d4-8ea2-0cb2386ad382",
    "_uuid": "d6e62d64e9d9aa70223576a1df91a008aa6c2664"
   },
   "source": [
    "## A Deeper Look into LogisticRegression:\n",
    "<a id=\"logistic\"></a>\n",
    "In this section we will ive a deeper look into the <b> logistic regression classifier</b>.\n",
    "\n",
    "\n",
    "### Terms:\n",
    "<ul>\n",
    "<li><b>True Positives:</b> Correctly Classified Fraud Transactions </li>\n",
    "<li><b>False Positives:</b> Incorrectly Classified Fraud Transactions</li>\n",
    "<li> <b>True Negative:</b> Correctly Classified Non-Fraud Transactions</li>\n",
    "<li> <b>False Negative:</b> Incorrectly Classified Non-Fraud Transactions</li>\n",
    "<li><b>Precision: </b>  True Positives/(True Positives + False Positives)  </li>\n",
    "<li><b> Recall: </b> True Positives/(True Positives + False Negatives)   </li>\n",
    "<li> Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.</li>\n",
    "<li><b>Precision/Recall Tradeoff: </b> The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect. </li>\n",
    "</ul>\n",
    "\n",
    "### Summary:\n",
    "<ul>\n",
    "<li> <b>Precision starts to descend</b> between 0.90 and 0.92 nevertheless, our precision score is still pretty high and still we have a descent recall score. </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4eaea18-ec79-4cb2-9a92-8d70a7f593bf",
    "_kg_hide-input": true,
    "_uuid": "0daaa7137ab61d6fd88e5fcc0849acc94c693df0"
   },
   "outputs": [],
   "source": [
    "def logistic_roc_curve(log_fpr, log_tpr):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('Logistic Regression ROC Curve', fontsize=16)\n",
    "    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.axis([-0.01,1,0,1])\n",
    "    \n",
    "    \n",
    "logistic_roc_curve(log_fpr, log_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d59c7621-5adb-4339-9a04-54630f679665",
    "_uuid": "f6d54ac036fa499104d269dd52d704c71629c1b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06d9e8b8-3ba5-4c1e-8480-af5a8e04f851",
    "_kg_hide-input": true,
    "_uuid": "b19df81d0a5178a260d7518f9cca804646839c01"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "y_pred = log_reg.predict(X_train)\n",
    "\n",
    "# Overfitting Case\n",
    "print('---' * 45)\n",
    "print('Overfitting: \\n')\n",
    "print('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\n",
    "print('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\n",
    "print('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n",
    "print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n",
    "print('---' * 45)\n",
    "\n",
    "# How it should look like\n",
    "print('---' * 45)\n",
    "print('How it should be:\\n')\n",
    "print(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\n",
    "print(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\n",
    "print(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\n",
    "print(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\n",
    "print('---' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89d88863-536f-4eea-a0cd-d2c77f407dd6",
    "_kg_hide-input": true,
    "_uuid": "f041ab92c183d2aa29569fc048ee6af4e6ee81f0"
   },
   "outputs": [],
   "source": [
    "undersample_y_score = log_reg.decision_function(original_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f54604f-396c-421e-ae93-305ad0103591",
    "_kg_hide-input": true,
    "_uuid": "c501d9226855a510a136bbf06794c702497e5b28"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "undersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      undersample_average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2442a51e-0263-48a3-99f9-06f47bdc04f1",
    "_kg_hide-input": true,
    "_uuid": "2edd5461ff5253f12955ac02106c323f7aabe49f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n",
    "\n",
    "plt.step(recall, precision, color='#004a93', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='#48a6ff')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
    "          undersample_average_precision), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e70e913a-173b-401b-96ee-93ddda7374c0",
    "_uuid": "d901eb00581cc890075a93d292935304e5b63355"
   },
   "source": [
    "### SMOTE Technique (Over-Sampling):\n",
    "<a id=\"smote\"></a>\n",
    "<img src=\"https://raw.githubusercontent.com/rikunert/SMOTE_visualisation/master/SMOTE_R_visualisation_3.png\", width=800>\n",
    "<b>SMOTE</b> stands for Synthetic Minority Over-sampling Technique.  Unlike Random UnderSampling, SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\". <br><br>\n",
    "\n",
    "\n",
    "<b> Understanding SMOTE: </b>\n",
    "<ul>\n",
    "<li> <b> Solving the Class Imbalance: </b> SMOTE creates synthetic points from the minority class in order to reach an equal balance between the minority and majority class. </li>\n",
    "<li><b>Location of the synthetic points: </b>   SMOTE picks the distance between the closest neighbors of the minority class, in between these distances it creates synthetic points. </li>\n",
    "<li> <b>Final Effect:  </b> More information is retained since we didn't have to delete any rows unlike in random undersampling.</li>\n",
    "<li><b> Accuracy || Time Tradeoff: </b> Although it is likely that SMOTE will be more accurate than random under-sampling, it will take more time to train since no rows are eliminated as previously stated.</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "### Cross Validation Overfitting Mistake:\n",
    "## Overfitting during Cross Validation:  \n",
    "In our undersample analysis I want to show you a common mistake I made that I want to share with all of you. It is simple, if you want to undersample or oversample your data you should not do it before cross validating. Why because you will be directly influencing the validation set before implementing cross-validation causing a \"data leakage\" problem. <b>In the following section you will see amazing precision and recall scores but in reality our data is overfitting!</b>\n",
    "### The Wrong Way:\n",
    "<img src=\"https://www.marcoaltini.com/uploads/1/3/2/3/13234002/2639934.jpg?401\"><br>\n",
    "\n",
    "As mentioned previously, if we get the minority class (\"Fraud) in our case, and create the synthetic points before cross validating we have a certain influence on the \"validation set\" of the cross validation process. Remember how cross validation works, let's assume we are splitting the data into 5 batches, 4/5 of the dataset will be the training set while 1/5 will be the validation set. The test set should not be touched! For that reason, we have to do the creation of synthetic datapoints \"during\" cross-validation and not before, just like below: <br>\n",
    "\n",
    "\n",
    "### The Right Way:\n",
    "<img src=\"https://www.marcoaltini.com/uploads/1/3/2/3/13234002/9101820.jpg?372\"> <br>\n",
    "As you see above, SMOTE occurs \"during\" cross validation and not \"prior\" to the cross validation process. Synthetic data are created only for the training set without affecting the validation set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**References**: \n",
    "<ul>\n",
    "<li><a src=\"https://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation\"> \n",
    "DEALING WITH IMBALANCED DATA: UNDERSAMPLING, OVERSAMPLING AND PROPER CROSS-VALIDATION </a></li> \n",
    "\n",
    "<li> <a src=\"http://rikunert.com/SMOTE_explained \"> SMOTE explained for noobs  </a></li>\n",
    "<li> <a src=\"https://www.youtube.com/watch?v=DQC_YE3I5ig&t=794s\"> Machine Learning - Over-& Undersampling - Python/ Scikit/ Scikit-Imblearn </a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc175ddc-ddd7-4087-ae1f-dd6fac664d58",
    "_kg_hide-input": true,
    "_uuid": "96f8d3f4160d65f12af4c7106739c4ad46d1e76b"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
    "\n",
    "# List to append the score and then find the average\n",
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "# Classifier with optimal parameters\n",
    "# log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
    "\n",
    "\n",
    "# Implementing SMOTE Technique \n",
    "# Cross Validating the right way\n",
    "# Parameters\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "for train, test in sss.split(original_Xtrain, original_ytrain):\n",
    "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
    "    best_est = rand_log_reg.best_estimator_\n",
    "    prediction = best_est.predict(original_Xtrain[test])\n",
    "    \n",
    "    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    precision_lst.append(precision_score(original_ytrain[test], prediction))\n",
    "    recall_lst.append(recall_score(original_ytrain[test], prediction))\n",
    "    f1_lst.append(f1_score(original_ytrain[test], prediction))\n",
    "    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n",
    "    \n",
    "print('---' * 45)\n",
    "print('')\n",
    "print(\"accuracy: {}\".format(np.mean(accuracy_lst)))\n",
    "print(\"precision: {}\".format(np.mean(precision_lst)))\n",
    "print(\"recall: {}\".format(np.mean(recall_lst)))\n",
    "print(\"f1: {}\".format(np.mean(f1_lst)))\n",
    "print('---' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "41dd6215-2927-4de3-999a-724272aea2b6",
    "_kg_hide-input": true,
    "_uuid": "d109652d1e170d0f9938d64f29aa33d93c941cdc"
   },
   "outputs": [],
   "source": [
    "labels = ['No Fraud', 'Fraud']\n",
    "smote_prediction = best_est.predict(original_Xtest)\n",
    "print(classification_report(original_ytest, smote_prediction, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f0e671f7-7ed1-4188-b9bf-e509f050b134",
    "_kg_hide-input": true,
    "_uuid": "a8dcc4bba95aed7fbc8b9e39ceeeec6902d1865c"
   },
   "outputs": [],
   "source": [
    "y_score = best_est.decision_function(original_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77bed8fa-1117-4bc0-a740-bd1bd97012a4",
    "_kg_hide-input": true,
    "_uuid": "f9213b24dd2fb3eb04f9b59c3b715dcb167664b5"
   },
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(original_ytest, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54e926f4-2a5d-4bb1-b74c-8cd79da7b6e5",
    "_kg_hide-input": true,
    "_uuid": "7be0445ac80df7ca252ec350b026d6275669aea6"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(original_ytest, y_score)\n",
    "\n",
    "plt.step(recall, precision, color='r', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='#F59B00')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
    "          average_precision), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5c6fe5b-f086-4151-aba5-3c758677be0f",
    "_kg_hide-input": true,
    "_uuid": "787ec6bb25c3dc379c12a57619f5cc3e41afa42e"
   },
   "outputs": [],
   "source": [
    "# SMOTE Technique (OverSampling) After splitting and Cross Validating\n",
    "sm = SMOTE(ratio='minority', random_state=42)\n",
    "# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "# This will be the data were we are going to \n",
    "Xsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7af62152-e7e3-45c8-9a56-69467ede59a6",
    "_kg_hide-input": true,
    "_uuid": "a25f7cc327bbaeae985cb0d2f9a0c8e2c2009aa3"
   },
   "outputs": [],
   "source": [
    "# We Improve the score by 2% points approximately \n",
    "# Implement GridSearchCV and the other models.\n",
    "\n",
    "# Logistic Regression\n",
    "t0 = time.time()\n",
    "log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm.fit(Xsm_train, ysm_train)\n",
    "t1 = time.time()\n",
    "print(\"Fitting oversample data took :{} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a250e819-cdd4-43f5-b0a4-eb8f232199a0",
    "_uuid": "feb07b601c9ec79be1fe96cbbadf4ac838f7f7a8"
   },
   "source": [
    "# Test Data with Logistic Regression:\n",
    "<a id=\"testing_logistic\"></a>\n",
    "## Confusion Matrix:\n",
    "**Positive/Negative:** Type of Class (label) [\"No\", \"Yes\"]\n",
    "**True/False:** Correctly or Incorrectly classified by the model.<br><br>\n",
    "\n",
    "**True Negatives (Top-Left Square):** This is the number of **correctly** classifications of the \"No\" (No Fraud Detected) class. <br><br>\n",
    "\n",
    "**False Negatives (Top-Right Square):** This is the number of **incorrectly** classifications of the \"No\"(No Fraud Detected) class. <br><br>\n",
    "\n",
    "**False Positives (Bottom-Left Square):** This is the number of **incorrectly** classifications of the \"Yes\" (Fraud Detected) class <br><br>\n",
    "\n",
    "**True Positives (Bottom-Right Square):** This is the number of **correctly** classifications of the \"Yes\" (Fraud Detected) class.\n",
    "\n",
    "\n",
    "### Summary: \n",
    "<ul>\n",
    "<li> <b>Random UnderSampling:</b> We will evaluate the final performance of the classification models in the random undersampling subset. <b>Keep in mind that this is not the data from the original dataframe. </b> </li>\n",
    "<li> <b>Classification Models: </b> The models that performed the best were <b>logistic regression </b> and <b>support vector classifier (SVM)</b>  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "13a7d31c-2586-4946-aaa3-60090cd5680b",
    "_kg_hide-input": true,
    "_uuid": "d0e37500506d1b942431ac5bfabedcfea30275ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Logistic Regression fitted using SMOTE technique\n",
    "y_pred_log_reg = log_reg_sm.predict(X_test)\n",
    "\n",
    "# Other models fitted with UnderSampling\n",
    "y_pred_knear = knears_neighbors.predict(X_test)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "\n",
    "\n",
    "log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\n",
    "kneighbors_cf = confusion_matrix(y_test, y_pred_knear)\n",
    "svc_cf = confusion_matrix(y_test, y_pred_svc)\n",
    "tree_cf = confusion_matrix(y_test, y_pred_tree)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2,figsize=(22,12))\n",
    "\n",
    "\n",
    "sns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\n",
    "ax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\n",
    "ax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\n",
    "ax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\n",
    "ax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\n",
    "ax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\n",
    "ax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\n",
    "ax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\n",
    "ax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd4529fd-f38a-4dd1-8b63-467a15a2167d",
    "_kg_hide-input": true,
    "_uuid": "1380d639d3b9087ec767ed6db391fc4b8c01e765"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "print('KNears Neighbors:')\n",
    "print(classification_report(y_test, y_pred_knear))\n",
    "\n",
    "print('Support Vector Classifier:')\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "print('Support Vector Classifier:')\n",
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9103c5ed-df9d-4441-91dc-1104a51f06ff",
    "_kg_hide-input": true,
    "_uuid": "49c94105ad280d1ca16271daf7f9395041016c5c"
   },
   "outputs": [],
   "source": [
    "# Final Score in the test set of logistic regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression with Under-Sampling\n",
    "y_pred = log_reg.predict(X_test)\n",
    "undersample_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\n",
    "y_pred_sm = best_est.predict(original_Xtest)\n",
    "oversample_score = accuracy_score(original_ytest, y_pred_sm)\n",
    "\n",
    "\n",
    "d = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\n",
    "final_df = pd.DataFrame(data=d)\n",
    "\n",
    "# Move column\n",
    "score = final_df['Score']\n",
    "final_df.drop('Score', axis=1, inplace=True)\n",
    "final_df.insert(1, 'Score', score)\n",
    "\n",
    "# Note how high is accuracy score it can be misleading! \n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ec3ca7c-fae2-4e0a-991b-bb689a8c4456",
    "_uuid": "d709eac19181e9b7026e2f9e9f780a207bc8c19a"
   },
   "source": [
    "## Neural Networks Testing Random UnderSampling Data vs OverSampling (SMOTE): \n",
    "<a id=\"neural_networks\"></a>\n",
    "In this section we will implement a simple Neural Network (with one hidden layer) in order to see  which of the two logistic regressions models we implemented in the (undersample or oversample(SMOTE)) has a better accuracy for detecting fraud and non-fraud transactions. <br><br>\n",
    "\n",
    "### Our Main Goal:\n",
    "Our main goal is to explore how our simple neural network behaves in both the random undersample and oversample dataframes and see whether they can predict accuractely both non-fraud and fraud cases. Why not only focus on fraud? Imagine you were a cardholder and after you purchased an item your card gets blocked because the bank's algorithm thought your purchase was a fraud. That's why we shouldn't emphasize only in detecting fraud cases but we should also emphasize correctly categorizing non-fraud transactions.\n",
    "\n",
    "\n",
    "### The Confusion Matrix:\n",
    "Here is again, how the confusion matrix works:\n",
    "<ul>\n",
    "<li><b>Upper Left Square: </b> The amount of <b>correctly</b> classified by our model of  no fraud transactions. </li>\n",
    "<li> <b>Upper Right Square:</b> The amount of  <b>incorrectly </b> classified transactions as fraud cases, but the actual label is <b> no fraud </b>. </li>\n",
    "<li><b>Lower Left Square:</b> The amount of <b> incorrectly </b> classified transactions as no fraud cases, but the actual label is <b>fraud </b>. </li>\n",
    "<li><b> Lower Right Square:</b> The amount of  <b>correctly</b> classified by our model of fraud transactions. </li>\n",
    "</ul>\n",
    "\n",
    "### Summary (Keras || Random UnderSampling):\n",
    "<ul>\n",
    "<li><b>Dataset: </b> In this final phase of testing we will fit this model in both the <b>random undersampled subset</b>  and <b> oversampled dataset (SMOTE) </b>in order to predict the final result using the <b>original dataframe testing data.</b> </li>\n",
    "<li>  <b>Neural Network Structure: </b> As stated previously, this will be a simple model composed of one input layer (where the number of nodes equals the number of features) plus bias node, one hidden layer with 32 nodes and one output node composed of two possible results 0 or 1 (No fraud or fraud). </li>\n",
    "<li> <b>Other characteristics:</b> The learning rate will be 0.001, the optimizer we will use is the AdamOptimizer, the activation function that is used in this scenario is \"Relu\" and for the final outputs we will use sparse categorical cross entropy, which gives the probability whether an instance case is no fraud or fraud (The prediction will pick the highest probability between the two.) </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e774e22e-8ce0-4c2e-99fa-9f3c6a915b6d",
    "_kg_hide-input": true,
    "_uuid": "35be99c61da4054c952e1955a5e809d003966975"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "\n",
    "undersample_model = Sequential([\n",
    "    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c249283c-f4d9-43f4-a859-ccd5a7661cf7",
    "_kg_hide-input": true,
    "_uuid": "ccdae6b84326551e1ff5199c44f7d53ccd3179d9"
   },
   "outputs": [],
   "source": [
    "undersample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6327357a-b8ca-4aa4-8764-48673b2d6c9d",
    "_kg_hide-input": true,
    "_uuid": "e2ec864b9ef6f530df28688a703bcc8f2243baa1"
   },
   "outputs": [],
   "source": [
    "undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0067f625-d734-4c15-9526-6efb1c47dc2c",
    "_kg_hide-input": true,
    "_uuid": "98a36722723d4f7285eb9de158b12be9694a603f"
   },
   "outputs": [],
   "source": [
    "undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a30cd49-23c3-4417-8ce1-b6e82890481c",
    "_kg_hide-input": true,
    "_uuid": "e82f40ef343b53b71d6fcfa317e872278db27114"
   },
   "outputs": [],
   "source": [
    "undersample_predictions = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a47a91a0-2cf3-436f-a1cd-bb207ba94997",
    "_kg_hide-input": true,
    "_uuid": "028270a5b2ba0e4c85100f287fec64343ad900ea"
   },
   "outputs": [],
   "source": [
    "undersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "44216511-5fa7-404f-9b40-2c72f30c1ca7",
    "_kg_hide-input": true,
    "_uuid": "b0681d10d7f3e68a6b91864670b7aa04cacd362f"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Create a confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee16d514-2134-4cfd-b4d8-1ddb183960f0",
    "_kg_hide-input": true,
    "_uuid": "003c84c96d49bebdb5f09970102d89e3db5ff2f1"
   },
   "outputs": [],
   "source": [
    "undersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\n",
    "actual_cm = confusion_matrix(original_ytest, original_ytest)\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "fig.add_subplot(221)\n",
    "plot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d7cd385-9270-426e-b537-c80501dce889",
    "_uuid": "be2b0e76445ecb745b6e49e69993dd1de7839eb4"
   },
   "source": [
    "### Keras || OverSampling (SMOTE):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7c29164-751a-4ccd-b517-527debf38fdf",
    "_kg_hide-input": true,
    "_uuid": "7130856ed8a6f87fe86b72c5142ff27ccf4eef1a"
   },
   "outputs": [],
   "source": [
    "n_inputs = Xsm_train.shape[1]\n",
    "\n",
    "oversample_model = Sequential([\n",
    "    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25beccc6-fe0b-4c85-a439-81aaf5cdc019",
    "_kg_hide-input": true,
    "_uuid": "937a0a57f3dae8172fb6c88b60944257e8198ae7"
   },
   "outputs": [],
   "source": [
    "oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "888c3acd-3d8d-4f1b-b68a-bef304feca14",
    "_kg_hide-input": true,
    "_uuid": "5ddbf33763fb33393d7969fdbd7338aa8e708c43"
   },
   "outputs": [],
   "source": [
    "oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09628790-3d4b-4516-b355-4177e5ad9329",
    "_kg_hide-input": true,
    "_uuid": "222a3f8e7614c4e0241e0f0c85b0ecd45fd3cca6"
   },
   "outputs": [],
   "source": [
    "oversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "374217c9-a4b7-4691-918b-6a91bb1c02e3",
    "_kg_hide-input": true,
    "_uuid": "9ede2c436251f560615087202cff031936f3a6e5"
   },
   "outputs": [],
   "source": [
    "oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a58d39f-9149-4279-bce3-fc8372e55f93",
    "_kg_hide-input": true,
    "_uuid": "a18452b7051e4905f32b27940f006d0bc4bc2d5e"
   },
   "outputs": [],
   "source": [
    "oversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\n",
    "actual_cm = confusion_matrix(original_ytest, original_ytest)\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "fig.add_subplot(221)\n",
    "plot_confusion_matrix(oversample_smote, labels, title=\"OverSample (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee0932c1-e9dc-4135-93af-1972d07d3d0f",
    "_uuid": "1b03ca4b3e37985bea49686abd466fdd9a7d84d3"
   },
   "source": [
    "### Conclusion: \n",
    "Implementing SMOTE on our imbalanced dataset helped us with the imbalance of our labels (more no fraud than fraud transactions). Nevertheless, I still have to state that sometimes the neural network on the oversampled dataset predicts less correct fraud transactions than our model using the undersample dataset. However, remember that the removal of outliers was implemented only on the random undersample dataset and not on the oversampled one. Also, in our undersample data our model is unable to detect for a large number of cases non fraud transactions correctly and instead, misclassifies those non fraud transactions as fraud cases. Imagine that people that were making regular purchases got their card blocked due to the reason that our model classified that transaction as a fraud transaction, this will be a huge disadvantage for the financial institution. The number of customer complaints and customer disatisfaction will increase.  The next step of this analysis will be to do an outlier removal on our oversample dataset and see if our accuracy in the test set improves. <br><br>\n",
    "\n",
    "**Note:** One last thing, predictions and accuracies may be subjected to change since I implemented data shuffling on both types of dataframes. The main thing is to see if our models are able to correctly classify no fraud and fraud transactions. I will bring more updates, stay tuned!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
